{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cfb71cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3a53f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95398/3223873231.py:1: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  allexp = pd.read_csv('../data/atlas_allexp.csv')\n"
     ]
    }
   ],
   "source": [
    "allexp = pd.read_csv('../data/atlas_allexp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef01258",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75e93057",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 2.0        # sliding window side length (in your coordinate units)\n",
    "grid_n   = 25           # grid resolution inside each window (grid_n x grid_n)\n",
    "bw       = 0.2          # KDE bandwidth for scipy gaussian_kde (float or 'scott'/'silverman')\n",
    "min_pts  = 5            # minimum points to fit a KDE for a class\n",
    "weight_mode = 'sum'  # 'sum' -> w_i = a_i + b_i ; 'prod' -> w_i = a_i * b_i\n",
    "eps        = 1e-12   # numerical floor to avoid 0-division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d64492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from scipy.stats import gaussian_kde, pearsonr\n",
    "\n",
    "def build_global_kdes(df, bw_method=bw, min_points=min_pts):\n",
    "    \"\"\"\n",
    "    Build a gaussian_kde per class using ALL points (global KDEs).\n",
    "    Returns: dict[class] -> fitted KDE\n",
    "    \"\"\"\n",
    "    kdes = {}\n",
    "    for ctype, sub in df.groupby('class'):\n",
    "        pts = sub[['x','y']].to_numpy()\n",
    "        if pts.shape[0] < min_points:\n",
    "            # Not enough points to build a stable KDE; skip this class\n",
    "            continue\n",
    "        kdes[ctype] = gaussian_kde(pts.T, bw_method=bw_method)\n",
    "    return kdes\n",
    "\n",
    "def sliding_windows(xmin, xmax, ymin, ymax, size):\n",
    "    \"\"\"\n",
    "    Yield windows (x0, x1, y0, y1) with stride size/2, covering [xmin,xmax]x[ymin,ymax].\n",
    "    The last window is clipped to remain within bounds.\n",
    "    \"\"\"\n",
    "    step = size / 2.0\n",
    "\n",
    "    def edges(a_min, a_max):\n",
    "        starts = []\n",
    "        cur = a_min\n",
    "        while cur + size <= a_max + 1e-9:\n",
    "            starts.append(cur)\n",
    "            cur += step\n",
    "        # ensure we end exactly at the boundary\n",
    "        if len(starts) == 0 or starts[-1] + size < a_max:\n",
    "            starts.append(max(a_min, a_max - size))\n",
    "        return sorted(set(starts))\n",
    "\n",
    "    xs = edges(xmin, xmax)\n",
    "    ys = edges(ymin, ymax)\n",
    "\n",
    "    for x0 in xs:\n",
    "        for y0 in ys:\n",
    "            yield (x0, x0 + size, y0, y0 + size)\n",
    "\n",
    "def grid_cell_centers(x0, x1, y0, y1, n):\n",
    "    \"\"\"\n",
    "    Return (GX, GY) mesh of grid cell centers (n x n) within the window [x0,x1]x[y0,y1].\n",
    "    \"\"\"\n",
    "    hx = (x1 - x0) / n\n",
    "    hy = (y1 - y0) / n\n",
    "    xs = x0 + hx * (np.arange(n) + 0.5)\n",
    "    ys = y0 + hy * (np.arange(n) + 0.5)\n",
    "    GX, GY = np.meshgrid(xs, ys, indexing='xy')\n",
    "    return GX, GY, hx, hy\n",
    "\n",
    "def windowwise_normalize(Z, hx, hy):\n",
    "    \"\"\"\n",
    "    Normalize a KDE patch so its discrete integral over the window is 1.\n",
    "    Z is n x n over centers; integral ≈ Z.sum() * hx * hy.\n",
    "    \"\"\"\n",
    "    mass = Z.sum() * hx * hy\n",
    "    return Z / mass if mass > 0 else Z\n",
    "\n",
    "def compute_pairwise_pcc_map(df):\n",
    "    \"\"\"\n",
    "    Main driver:\n",
    "      - builds global KDEs per class\n",
    "      - iterates sliding windows\n",
    "      - for each class pair, evaluates KDEs on window grid, normalizes within window, computes PCC\n",
    "      - returns a results DataFrame\n",
    "    \"\"\"\n",
    "    # Bounds for windows\n",
    "    xmin, xmax = df['x'].min(), df['x'].max()\n",
    "    ymin, ymax = df['y'].min(), df['y'].max()\n",
    "\n",
    "    # Global KDEs\n",
    "    kdes = build_global_kdes(df)\n",
    "    classes = sorted(kdes.keys())\n",
    "    pairs = list(combinations(classes, 2))\n",
    "\n",
    "    records = []\n",
    "\n",
    "    # Slide windows\n",
    "    for (x0, x1, y0, y1) in sliding_windows(xmin, xmax, ymin, ymax, win_size):\n",
    "        # grid of cell centers for this window\n",
    "        GX, GY, hx, hy = grid_cell_centers(x0, x1, y0, y1, grid_n)\n",
    "        XY = np.vstack([GX.ravel(), GY.ravel()])\n",
    "\n",
    "        for A, B in pairs:\n",
    "            kdeA = kdes.get(A)\n",
    "            kdeB = kdes.get(B)\n",
    "            if kdeA is None or kdeB is None:\n",
    "                # One of the KDEs wasn't available (too few points)\n",
    "                continue\n",
    "\n",
    "            # Evaluate KDEs on grid centers inside this window\n",
    "            ZA = kdeA(XY).reshape(GX.shape)\n",
    "            ZB = kdeB(XY).reshape(GX.shape)\n",
    "\n",
    "            # Window-wise normalization (probability over the window)\n",
    "            ZA = windowwise_normalize(ZA, hx, hy)\n",
    "            ZB = windowwise_normalize(ZB, hx, hy)\n",
    "\n",
    "            # Pearson correlation across grid cells (flatten)\n",
    "            a = ZA.ravel()\n",
    "            b = ZB.ravel()\n",
    "\n",
    "            # If either map is constant, pearsonr is undefined; handle gracefully\n",
    "            if np.allclose(a, a[0]) or np.allclose(b, b[0]):\n",
    "                r = np.nan\n",
    "                p = np.nan\n",
    "            else:\n",
    "                r, p = pearsonr(a, b)\n",
    "\n",
    "            records.append({\n",
    "                'x0': x0, 'x1': x1, 'y0': y0, 'y1': y1,\n",
    "                'class_A': A, 'class_B': B,\n",
    "                'pearson_r': r, 'p_value': p,\n",
    "                'grid_n': grid_n\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03348892",
   "metadata": {},
   "source": [
    "# Unweighted PCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888f251",
   "metadata": {},
   "source": [
    "## PCC calcuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e3150",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = compute_pairwise_pcc_map(allexp)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b06d5",
   "metadata": {},
   "source": [
    "# Weighted PCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019bd0dd",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d8d2fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from scipy.stats import gaussian_kde, pearsonr\n",
    "\n",
    "def build_global_kdes(df, bw_method=bw, min_points=min_pts):\n",
    "    \"\"\"Fit a global gaussian_kde per class using all points (avoids edge bias).\"\"\"\n",
    "    kdes = {}\n",
    "    for ctype, sub in df.groupby('class'):\n",
    "        pts = sub[['x','y']].to_numpy()\n",
    "        if pts.shape[0] >= min_points:\n",
    "            kdes[ctype] = gaussian_kde(pts.T, bw_method=bw_method)\n",
    "    return kdes\n",
    "\n",
    "def sliding_windows(xmin, xmax, ymin, ymax, size):\n",
    "    \"\"\"Yield (x0,x1,y0,y1) with stride size/2, clipped to bounds.\"\"\"\n",
    "    step = size / 2.0\n",
    "    def starts(lo, hi):\n",
    "        s = []\n",
    "        cur = lo\n",
    "        while cur + size <= hi + 1e-9:\n",
    "            s.append(cur); cur += step\n",
    "        if not s or s[-1] + size < hi:  # ensure coverage to the edge\n",
    "            s.append(max(lo, hi - size))\n",
    "        return sorted(set(s))\n",
    "    for x0 in starts(xmin, xmax):\n",
    "        for y0 in starts(ymin, ymax):\n",
    "            yield (x0, x0 + size, y0, y0 + size)\n",
    "\n",
    "def grid_cell_centers(x0, x1, y0, y1, n):\n",
    "    \"\"\"n x n mesh of grid cell centers inside the window + cell sizes (hx, hy).\"\"\"\n",
    "    hx = (x1 - x0) / n\n",
    "    hy = (y1 - y0) / n\n",
    "    xs = x0 + hx * (np.arange(n) + 0.5)\n",
    "    ys = y0 + hy * (np.arange(n) + 0.5)\n",
    "    GX, GY = np.meshgrid(xs, ys, indexing='xy')\n",
    "    return GX, GY, hx, hy\n",
    "\n",
    "def windowwise_normalize(Z, hx, hy):\n",
    "    \"\"\"Rescale KDE patch so its discrete integral in the window is 1.\"\"\"\n",
    "    mass = Z.sum() * hx * hy\n",
    "    return Z / mass if mass > 0 else Z\n",
    "\n",
    "def weighted_pearson(a, b, mode='sum', eps=1e-12):\n",
    "    \"\"\"\n",
    "    Weighted Pearson correlation between arrays a, b (same shape).\n",
    "    mode='sum' uses w=a+b; mode='prod' uses w=a*b.\n",
    "    Returns np.nan if variance is zero or total weight ~0.\n",
    "    \"\"\"\n",
    "    a = np.asarray(a).ravel()\n",
    "    b = np.asarray(b).ravel()\n",
    "\n",
    "    if mode == 'prod':\n",
    "        w = a * b\n",
    "    else:\n",
    "        w = a + b\n",
    "\n",
    "    w_sum = w.sum()\n",
    "    if not np.isfinite(w_sum) or w_sum <= eps:\n",
    "        return np.nan\n",
    "\n",
    "    w = w / w_sum  # normalize weights to sum to 1 (probability weights)\n",
    "\n",
    "    mu_a = (w * a).sum()\n",
    "    mu_b = (w * b).sum()\n",
    "\n",
    "    da = a - mu_a\n",
    "    db = b - mu_b\n",
    "\n",
    "    var_a = (w * da * da).sum()\n",
    "    var_b = (w * db * db).sum()\n",
    "    if var_a <= eps or var_b <= eps:\n",
    "        return np.nan\n",
    "\n",
    "    cov_ab = (w * da * db).sum()\n",
    "    return cov_ab / np.sqrt(var_a * var_b)\n",
    "\n",
    "def compute_pairwise_weighted_pcc(df):\n",
    "    \"\"\"\n",
    "    - Fit global KDEs per class.\n",
    "    - Slide window; make a grid of centers; evaluate KDEs.\n",
    "    - Window-wise normalize each KDE patch.\n",
    "    - Compute **weighted** Pearson r for each class pair in each window.\n",
    "    \"\"\"\n",
    "    xmin, xmax = df['x'].min(), df['x'].max()\n",
    "    ymin, ymax = df['y'].min(), df['y'].max()\n",
    "\n",
    "    kdes = build_global_kdes(df)\n",
    "    classes = sorted(kdes.keys())\n",
    "    pairs = list(combinations(classes, 2))\n",
    "\n",
    "    records = []\n",
    "    for (x0, x1, y0, y1) in sliding_windows(xmin, xmax, ymin, ymax, win_size):\n",
    "        GX, GY, hx, hy = grid_cell_centers(x0, x1, y0, y1, grid_n)\n",
    "        XY = np.vstack([GX.ravel(), GY.ravel()])\n",
    "\n",
    "        for A, B in pairs:\n",
    "            kdeA = kdes.get(A); kdeB = kdes.get(B)\n",
    "            if kdeA is None or kdeB is None:\n",
    "                continue\n",
    "\n",
    "            ZA = kdeA(XY).reshape(GX.shape)\n",
    "            ZB = kdeB(XY).reshape(GX.shape)\n",
    "\n",
    "            # Window-wise normalization (probability maps over this window)\n",
    "            ZA = windowwise_normalize(ZA, hx, hy)\n",
    "            ZB = windowwise_normalize(ZB, hx, hy)\n",
    "\n",
    "            # Weighted PCC so low-signal cells don't dominate\n",
    "            r_w = weighted_pearson(ZA, ZB, mode=weight_mode, eps=eps)\n",
    "\n",
    "            records.append({\n",
    "                'x0': x0, 'x1': x1, 'y0': y0, 'y1': y1,\n",
    "                'class_A': A, 'class_B': B,\n",
    "                'weighted_pearson_r': r_w,\n",
    "                'grid_n': grid_n, 'win_size': win_size,\n",
    "                'weight_mode': weight_mode\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame.from_records(records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612691fe",
   "metadata": {},
   "source": [
    "## PCC Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = compute_pairwise_weighted_pcc(allexp)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c57ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('../data/pairwise_weighted_pcc_map.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640d451d",
   "metadata": {},
   "source": [
    "## Optional: Load Formed Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c276c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('../data/pairwise_weighted_pcc_map.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055574b0",
   "metadata": {},
   "source": [
    "# Island Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac87df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers\n",
    "# -------------------------------\n",
    "\n",
    "def fisher_z(r, eps=1e-12):\n",
    "    \"\"\"Fisher z-transform with safe clipping.\"\"\"\n",
    "    r = np.asarray(r, dtype=float)\n",
    "    r = np.where(np.isfinite(r), r, np.nan)\n",
    "    r = np.clip(r, -1 + eps, 1 - eps)\n",
    "    # if r is a scalar, np.arctanh returns a scalar\n",
    "    # make r an array to ensure z is also an array\n",
    "    z = np.arctanh(r)\n",
    "    # if z is infinite, set to nan\n",
    "    if np.isinf(z):\n",
    "        z = np.nan\n",
    "    return z\n",
    "\n",
    "def _build_pair_grids(results_df, A, B, r_col='weighted_pearson_r'):\n",
    "    \"\"\"\n",
    "    From a long-form results_df -> 2D grids aligned on unique (x0,y0).\n",
    "    Returns dict with r_grid, z_grid, and x0/x1/y0/y1 grids.\n",
    "    \"\"\"\n",
    "    sub = results_df[(results_df['class_A'] == A) & (results_df['class_B'] == B)].copy()\n",
    "    if sub.empty:\n",
    "        return None\n",
    "\n",
    "    # Ensure uniqueness if code was run multiple times\n",
    "    sub = sub.drop_duplicates(subset=['x0','y0','x1','y1','class_A','class_B'])\n",
    "\n",
    "    xs = np.array(sorted(sub['x0'].unique()))\n",
    "    ys = np.array(sorted(sub['y0'].unique()))\n",
    "\n",
    "    ix = {x0:i for i,x0 in enumerate(xs)}\n",
    "    iy = {y0:i for i,y0 in enumerate(ys)}\n",
    "\n",
    "    H, W = len(ys), len(xs)\n",
    "    r_grid  = np.full((H, W), np.nan, dtype=float)\n",
    "    z_grid  = np.full((H, W), np.nan, dtype=float)\n",
    "    x0_grid = np.full((H, W), np.nan, dtype=float)\n",
    "    x1_grid = np.full((H, W), np.nan, dtype=float)\n",
    "    y0_grid = np.full((H, W), np.nan, dtype=float)\n",
    "    y1_grid = np.full((H, W), np.nan, dtype=float)\n",
    "\n",
    "    for _, row in sub.iterrows():\n",
    "        i = iy[row['y0']]\n",
    "        j = ix[row['x0']]\n",
    "        r = row.get(r_col, np.nan)\n",
    "        r_grid[i, j] = r\n",
    "        z_grid[i, j] = fisher_z(r)\n",
    "        x0_grid[i, j] = row['x0']; x1_grid[i, j] = row['x1']\n",
    "        y0_grid[i, j] = row['y0']; y1_grid[i, j] = row['y1']\n",
    "\n",
    "    return {\n",
    "        'xs': xs, 'ys': ys,\n",
    "        'r_grid': r_grid, 'z_grid': z_grid,\n",
    "        'x0_grid': x0_grid, 'x1_grid': x1_grid,\n",
    "        'y0_grid': y0_grid, 'y1_grid': y1_grid\n",
    "    }\n",
    "\n",
    "def _label_8_connected(mask):\n",
    "    \"\"\"8-connected component labeling (NaNs already excluded in mask).\"\"\"\n",
    "    structure = np.ones((3,3), dtype=int)   # 8-connectivity\n",
    "    labels, n = ndimage.label(mask, structure=structure)\n",
    "    return labels, n\n",
    "\n",
    "# -------------------------------\n",
    "# Main: find islands for all pairs\n",
    "# -------------------------------\n",
    "\n",
    "def find_islands_for_all_pairs(\n",
    "    results_df,\n",
    "    r_threshold=0.5,           # threshold in r; internally converted to z\n",
    "    r_col='weighted_pearson_r',\n",
    "    min_windows=1              # drop tiny islands if desired\n",
    "):\n",
    "    \"\"\"\n",
    "    Build 8-connected 'islands' per (class_A, class_B).\n",
    "    Returns: list of island dicts + an index table for quick lookup.\n",
    "    \"\"\"\n",
    "    islands = []\n",
    "    index_rows = []\n",
    "\n",
    "    z_thr = float(fisher_z(r_threshold))\n",
    "\n",
    "    pairs = (\n",
    "        results_df[['class_A','class_B']]\n",
    "        .drop_duplicates()\n",
    "        .sort_values(['class_A','class_B'])\n",
    "        .itertuples(index=False, name=None)\n",
    "    )\n",
    "\n",
    "    for (A, B) in pairs:\n",
    "        grids = _build_pair_grids(results_df, A, B, r_col=r_col)\n",
    "        if grids is None:\n",
    "            continue\n",
    "\n",
    "        rG = grids['r_grid']; zG = grids['z_grid']\n",
    "        # Valid if r is finite AND above threshold\n",
    "        valid = np.isfinite(rG) & (rG > r_threshold)\n",
    "\n",
    "        if not np.any(valid):\n",
    "            # No islands at this pair\n",
    "            continue\n",
    "\n",
    "        labels, nlab = _label_8_connected(valid)\n",
    "\n",
    "        for lab in range(1, nlab+1):\n",
    "            mask = (labels == lab)\n",
    "            size = int(mask.sum())\n",
    "            if size < min_windows:\n",
    "                continue\n",
    "\n",
    "            # Extract stats\n",
    "            r_vals = rG[mask]\n",
    "            z_vals = zG[mask]\n",
    "\n",
    "            # Cluster \"mass\" above threshold in z-space (strength + extent)\n",
    "            cluster_mass = float(np.nansum(z_vals - z_thr))\n",
    "\n",
    "            # Spatial bbox from window rectangles\n",
    "            x0_min = float(np.nanmin(grids['x0_grid'][mask]))\n",
    "            x1_max = float(np.nanmax(grids['x1_grid'][mask]))\n",
    "            y0_min = float(np.nanmin(grids['y0_grid'][mask]))\n",
    "            y1_max = float(np.nanmax(grids['y1_grid'][mask]))\n",
    "\n",
    "            # Collect window rects (optional, handy for downstream)\n",
    "            # (x0,x1,y0,y1) list for all member windows\n",
    "            # NOTE: if you want to keep indices instead, you can store np.argwhere(mask)\n",
    "            member_rects = np.column_stack([\n",
    "                grids['x0_grid'][mask],\n",
    "                grids['x1_grid'][mask],\n",
    "                grids['y0_grid'][mask],\n",
    "                grids['y1_grid'][mask],\n",
    "            ]).tolist()\n",
    "\n",
    "            island = {\n",
    "                'pair': (A, B),\n",
    "                'label': lab,\n",
    "                'n_windows': size,\n",
    "                'median_r': float(np.nanmedian(r_vals)),\n",
    "                'mean_r': float(np.nanmean(r_vals)),\n",
    "                'max_r': float(np.nanmax(r_vals)),\n",
    "                'median_z': float(np.nanmedian(z_vals)),\n",
    "                'cluster_mass_z': cluster_mass,\n",
    "                'bbox': (x0_min, x1_max, y0_min, y1_max),\n",
    "                'window_rects': member_rects,\n",
    "                'grid_shape': rG.shape,\n",
    "                'grid_x0s': grids['xs'].tolist(),\n",
    "                'grid_y0s': grids['ys'].tolist(),\n",
    "                # Placeholders for later inference / stability:\n",
    "                'cluster_p': None,      # TODO: fill via permutation-based max-cluster test\n",
    "                'stability': None       # TODO: fill via bootstrap frequency / IoU\n",
    "            }\n",
    "            islands.append(island)\n",
    "            index_rows.append({\n",
    "                'class_A': A, 'class_B': B, 'label': lab,\n",
    "                'n_windows': size,\n",
    "                'bbox_x0': x0_min, 'bbox_x1': x1_max,\n",
    "                'bbox_y0': y0_min, 'bbox_y1': y1_max,\n",
    "                'cluster_mass_z': cluster_mass,\n",
    "                'median_r': island['median_r']\n",
    "            })\n",
    "\n",
    "    # Lightweight index DataFrame for quick filtering/sorting\n",
    "    island_index = pd.DataFrame(index_rows).sort_values(\n",
    "        ['class_A','class_B','cluster_mass_z','n_windows'],\n",
    "        ascending=[True, True, False, False]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return islands, island_index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37318eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "islands, island_index = find_islands_for_all_pairs(results_df,\n",
    "                                                   r_threshold=0.7,\n",
    "                                                   r_col='weighted_pearson_r',\n",
    "                                                   min_windows=4)\n",
    "island_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c60382a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "islands[1]['window_rects'][0]\n",
    "wx1 = islands[1]['window_rects'][0][0]\n",
    "wx2 = islands[1]['window_rects'][0][1]\n",
    "wy1 = islands[1]['window_rects'][0][2]\n",
    "wy2 = islands[1]['window_rects'][0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5b45319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Build a fast lookup of island windows per (A,B) as set of (x0,y0)\n",
    "# -------------------------------------------------------------\n",
    "def _island_windows_by_pair(islands):\n",
    "    pair_to_windows = defaultdict(set)\n",
    "    pair_to_axes = {}\n",
    "    for isl in islands:\n",
    "        A, B = isl['pair']\n",
    "        # store grid axes (assumes consistent grid across islands; OK if repeated)\n",
    "        pair_to_axes[(A,B)] = (np.array(isl['grid_x0s']), np.array(isl['grid_y0s']))\n",
    "        for (x0,x1,y0,y1) in isl['window_rects']:\n",
    "            pair_to_windows[(A,B)].add((float(x0), float(y0)))\n",
    "    return pair_to_windows, pair_to_axes\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Extract the global window grid (xs, ys) and win_size from results_df\n",
    "# (assumes a consistent grid used when computing r maps)\n",
    "# -------------------------------------------------------------\n",
    "def _extract_grid_from_results(results_df):\n",
    "    xs = np.array(sorted(results_df['x0'].unique()), dtype=float)\n",
    "    ys = np.array(sorted(results_df['y0'].unique()), dtype=float)\n",
    "    # infer win_size from first row\n",
    "    r0 = results_df.iloc[0]\n",
    "    win_size_x = float(r0['x1'] - r0['x0'])\n",
    "    win_size_y = float(r0['y1'] - r0['y0'])\n",
    "    assert np.isclose(win_size_x, win_size_y), \"Non-square windows not supported here.\"\n",
    "    return xs, ys, win_size_x\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Find all window (x0,y0) that cover a given point (x,y)\n",
    "# -------------------------------------------------------------\n",
    "def _covering_windows(x, y, xs, ys, win_size):\n",
    "    # windows with x0 <= x < x0+win_size and y0 <= y < y0+win_size\n",
    "    x_mask = (xs <= x) & (x < xs + win_size)\n",
    "    y_mask = (ys <= y) & (y < ys + win_size)\n",
    "    xi = np.where(x_mask)[0]\n",
    "    yi = np.where(y_mask)[0]\n",
    "    # Cartesian product of indices\n",
    "    return [(float(xs[j]), float(ys[i])) for i in yi for j in xi]\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Main: per-cell binary encoding + auxiliary table with coverage\n",
    "# -------------------------------------------------------------\n",
    "def encode_cell_colocalization(allexp, results_df, islands, theta=0.5, k=3, cell_colname='class'):\n",
    "    \"\"\"\n",
    "    For each cell of type A:\n",
    "      - collect all sliding windows covering the cell (support)\n",
    "      - for each partner B != A: compute coverage = (#covering windows that are island windows for (A,B)) / (#covering windows)\n",
    "      - set binary 1 if coverage >= theta and support >= k; else 0\n",
    "    Returns:\n",
    "      binary_mat: DataFrame [n_cells x n_types]\n",
    "      aux:        DataFrame with per-cell diagnostics (support, per-partner coverage)\n",
    "    \"\"\"\n",
    "    xs, ys, win_size = _extract_grid_from_results(results_df)\n",
    "    pair_to_windows, _ = _island_windows_by_pair(islands)\n",
    "\n",
    "    cell_types = sorted(allexp[cell_colname].unique())\n",
    "    n = len(allexp)\n",
    "\n",
    "    # Prepare outputs\n",
    "    bin_data = {t: np.zeros(n, dtype=int) for t in cell_types}  # self-col will stay 0\n",
    "    aux_rows = []\n",
    "\n",
    "    # Precompute for speed: all covering windows per cell\n",
    "    # (keeps exact geometry; typical stride = win_size/2 => up to 4 windows per cell)\n",
    "    all_cover = []\n",
    "    for idx, row in allexp[['x','y']].iterrows():\n",
    "        cov = _covering_windows(float(row['x']), float(row['y']), xs, ys, win_size)\n",
    "        all_cover.append(cov)\n",
    "\n",
    "    # Compute encoding\n",
    "    for idx, row in allexp.iterrows():\n",
    "        A = row[cell_colname]\n",
    "        covered = all_cover[idx]\n",
    "        support = len(covered)\n",
    "\n",
    "        # Per-partner coverage tracker\n",
    "        cov_map = {}\n",
    "\n",
    "        if support >= k:\n",
    "            covered_set = set(covered)\n",
    "            for B in cell_types:\n",
    "                if B == A:\n",
    "                    cov_map[B] = 0.0\n",
    "                    continue\n",
    "                # Island windows for pair (A,B) (order-agnostic lookup)\n",
    "                key = (A,B) if (A,B) in pair_to_windows else (B,A)\n",
    "                if key not in pair_to_windows:\n",
    "                    cov_map[B] = 0.0\n",
    "                    continue\n",
    "                island_windows = pair_to_windows[key]\n",
    "                hit = len(covered_set & island_windows)\n",
    "                coverage = hit / support if support > 0 else 0.0\n",
    "                cov_map[B] = coverage\n",
    "                # binary decision\n",
    "                if coverage >= theta:\n",
    "                    bin_data[B][idx] = 1\n",
    "        else:\n",
    "            # insufficient support: keep zeros, record coverage as 0 for all partners\n",
    "            for B in cell_types:\n",
    "                cov_map[B] = 0.0\n",
    "\n",
    "        # Build aux row\n",
    "        aux_row = {\n",
    "            'cell_index': idx,\n",
    "            'x': float(row['x']),\n",
    "            'y': float(row['y']),\n",
    "            'cell_type': A,\n",
    "            'support_windows': support\n",
    "        }\n",
    "        # add per-partner coverages (e.g., coverage_B)\n",
    "        for B in cell_types:\n",
    "            if B == A:\n",
    "                aux_row[f'coverage_{B}'] = 0.0\n",
    "            else:\n",
    "                aux_row[f'coverage_{B}'] = float(cov_map[B])\n",
    "        aux_rows.append(aux_row)\n",
    "\n",
    "    binary_mat = pd.DataFrame(bin_data, index=allexp.index)\n",
    "    # enforce self-col = 0 (safety)\n",
    "    for A in cell_types:\n",
    "        binary_mat.loc[allexp[cell_colname] == A, A] = 0\n",
    "\n",
    "    aux = pd.DataFrame(aux_rows).set_index('cell_index').loc[allexp.index]\n",
    "\n",
    "    return binary_mat, aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a90b1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "binary_mat, aux = encode_cell_colocalization(allexp, results_df, islands, theta=0.5, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bdc2e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_mat['cell_label'] = allexp['cell_label'].values\n",
    "aux['cell_label'] = allexp['cell_label'].values\n",
    "# make cell_label index\n",
    "binary_mat = binary_mat.set_index('cell_label')\n",
    "aux = aux.set_index('cell_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2cb964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep allexp columns from 22 to end except last 50\n",
    "allexp_sub = allexp.iloc[:, 22:-50]\n",
    "\n",
    "allexp_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c9ff7",
   "metadata": {},
   "source": [
    "# Feature Matrix Formation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc2a28",
   "metadata": {},
   "source": [
    "## Ligand-Receptor Genes Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8deb872",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_pairs = pd.read_csv('../data/mouse_850_lr_pairs_cpdb_interactions.csv')\n",
    "LR_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1366299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def prepare_lr_features(\n",
    "    allexp: pd.DataFrame,\n",
    "    lr_pairs: pd.DataFrame,\n",
    "    ligand_col: str = \"ligand_genesymbol\",   # adjust if your column names differ\n",
    "    receptor_col: str = \"target_genesymbol\", \n",
    "    meta_cols = (\"x\",\"y\",\"class\")            # non-gene columns in allexp to ignore\n",
    "):\n",
    "    \"\"\"\n",
    "    Build per-cell receptor and ligand expression matrices for exactly the LR pairs provided.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    allexp : DataFrame\n",
    "        Rows = cells (index = your cell ids/labels), columns include gene expression + meta columns.\n",
    "    lr_pairs : DataFrame\n",
    "        Must contain columns with ligand and receptor gene symbols (one row per LR pair).\n",
    "    ligand_col, receptor_col : str\n",
    "        Column names in lr_pairs for ligand and receptor symbols.\n",
    "    meta_cols : iterable\n",
    "        Columns in allexp that are NOT genes (will be excluded).\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    X_receptors : DataFrame  (cells × unique_receptors_kept)\n",
    "    X_ligands   : DataFrame  (cells × unique_ligands_kept)\n",
    "    lr_pairs_kept : DataFrame (filtered to pairs present in allexp, with integer columns\n",
    "                     'ligand_idx' and 'receptor_idx' giving column positions in X_ligands/X_receptors)\n",
    "    report : dict  (counts and lists of dropped/mapped genes)\n",
    "    \"\"\"\n",
    "    # --- sanitize / standardize gene symbols (upper-case) to improve matching ---\n",
    "    def _upper_series(s):\n",
    "        return s.astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Make a copy and upper-case gene names in allexp columns (genes only)\n",
    "    allexp_cols = pd.Index(allexp.columns)\n",
    "    meta_cols = [c for c in meta_cols if c in allexp_cols]\n",
    "    gene_cols = [c for c in allexp_cols if c not in meta_cols]\n",
    "\n",
    "    # Build a mapping original->UPPER for allexp gene columns\n",
    "    gene_cols_upper = pd.Index([str(c).upper() for c in gene_cols])\n",
    "    colmap = dict(zip(gene_cols_upper, gene_cols))  # UPPER -> original\n",
    "\n",
    "    # Upper-case ligand/receptor symbols\n",
    "    lig_syms = _upper_series(lr_pairs[ligand_col])\n",
    "    rec_syms = _upper_series(lr_pairs[receptor_col])\n",
    "\n",
    "    # Compose a filtered LR table with upper-cased symbols\n",
    "    lr_uc = lr_pairs.copy()\n",
    "    lr_uc[\"_LIG\"] = lig_syms\n",
    "    lr_uc[\"_REC\"] = rec_syms\n",
    "\n",
    "    # --- keep only pairs whose both genes are present in allexp ---\n",
    "    # present_lig = gene_cols_upper.isin(lr_uc[\"_LIG\"]).to_numpy()\n",
    "    # present_rec = gene_cols_upper.isin(lr_uc[\"_REC\"]).to_numpy()\n",
    "    genes_in_allexp_uc = set(gene_cols_upper)\n",
    "\n",
    "    keep_mask = lr_uc[\"_LIG\"].isin(genes_in_allexp_uc) & lr_uc[\"_REC\"].isin(genes_in_allexp_uc)\n",
    "    lr_pairs_kept = lr_uc.loc[keep_mask].reset_index(drop=True)\n",
    "\n",
    "    # Unique ligands/receptors actually present\n",
    "    uniq_lig_uc = list(dict.fromkeys(lr_pairs_kept[\"_LIG\"]))  # preserve order of first appearance\n",
    "    uniq_rec_uc = list(dict.fromkeys(lr_pairs_kept[\"_REC\"]))\n",
    "\n",
    "    # Map back to original column names in allexp\n",
    "    uniq_lig_cols = [colmap[g] for g in uniq_lig_uc]\n",
    "    uniq_rec_cols = [colmap[g] for g in uniq_rec_uc]\n",
    "\n",
    "    # --- slice allexp into ligand/receptor matrices (cells × genes) ---\n",
    "    X_ligands = allexp.loc[:, uniq_lig_cols].copy()\n",
    "    X_receptors = allexp.loc[:, uniq_rec_cols].copy()\n",
    "\n",
    "    # Optional: ensure numeric dtype\n",
    "    X_ligands = X_ligands.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X_receptors = X_receptors.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # --- annotate lr_pairs_kept with column indices into X_ligands / X_receptors ---\n",
    "    lig_idx_map = {g: i for i, g in enumerate(uniq_lig_cols)}\n",
    "    rec_idx_map = {g: i for i, g in enumerate(uniq_rec_cols)}\n",
    "\n",
    "    lr_pairs_kept[\"ligand_symbol_uc\"] = lr_pairs_kept[\"_LIG\"]\n",
    "    lr_pairs_kept[\"receptor_symbol_uc\"] = lr_pairs_kept[\"_REC\"]\n",
    "    lr_pairs_kept[\"ligand_symbol\"] = lr_pairs_kept[\"_LIG\"].map(colmap)\n",
    "    lr_pairs_kept[\"receptor_symbol\"] = lr_pairs_kept[\"_REC\"].map(colmap)\n",
    "    lr_pairs_kept[\"ligand_idx\"] = lr_pairs_kept[\"ligand_symbol\"].map(lig_idx_map)\n",
    "    lr_pairs_kept[\"receptor_idx\"] = lr_pairs_kept[\"receptor_symbol\"].map(rec_idx_map)\n",
    "\n",
    "    # --- reporting ---\n",
    "    dropped_pairs = lr_uc.loc[~keep_mask, [ligand_col, receptor_col]]\n",
    "    missing_ligs = sorted(set(lr_uc[\"_LIG\"]) - genes_in_allexp_uc)\n",
    "    missing_recs = sorted(set(lr_uc[\"_REC\"]) - genes_in_allexp_uc)\n",
    "\n",
    "    report = {\n",
    "        \"n_pairs_input\": int(len(lr_pairs)),\n",
    "        \"n_pairs_kept\": int(len(lr_pairs_kept)),\n",
    "        \"n_unique_ligands_kept\": int(len(uniq_lig_cols)),\n",
    "        \"n_unique_receptors_kept\": int(len(uniq_rec_cols)),\n",
    "        \"missing_ligands_from_allexp\": [colmap.get(g, g) for g in missing_ligs],  # best effort\n",
    "        \"missing_receptors_from_allexp\": [colmap.get(g, g) for g in missing_recs],\n",
    "        \"dropped_pairs\": dropped_pairs\n",
    "    }\n",
    "\n",
    "    # Clean columns for return\n",
    "    lr_pairs_kept = lr_pairs_kept.drop(columns=[\"_LIG\",\"_REC\"])\n",
    "\n",
    "    return X_receptors, X_ligands, lr_pairs_kept, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36f957fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Example usage:\n",
    "X_receptors, X_ligands, lr_pairs_kept, report = prepare_lr_features(allexp, LR_pairs,\n",
    "    ligand_col=\"ligand_genesymbol\", receptor_col=\"target_genesymbol\", meta_cols=[])\n",
    "# ---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d2f93",
   "metadata": {},
   "source": [
    "## Ligand Exposure Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27930b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Reuse your helpers (already defined above) -----------------------------\n",
    "# _extract_grid_from_results, _island_windows_by_pair, _covering_windows\n",
    "\n",
    "def _preindex_window_cells(allexp, xs, ys, win_size, class_col=\"class\"):\n",
    "    \"\"\"\n",
    "    Pre-index cells by (x0,y0) window and by cell type for fast neighbor lookup.\n",
    "    Returns: dict[(x0,y0)] -> dict[class_name] -> np.array(cell_indices)\n",
    "    \"\"\"\n",
    "    x = allexp[\"x\"].to_numpy(float)\n",
    "    y = allexp[\"y\"].to_numpy(float)\n",
    "    classes = allexp[class_col].astype(str).to_numpy()\n",
    "\n",
    "    # For each window start (x0,y0), build a boolean mask of cells inside it\n",
    "    win_index = {}\n",
    "    for x0 in xs:\n",
    "        x1 = x0 + win_size\n",
    "        x_mask = (x >= x0) & (x < x1)\n",
    "        for y0 in ys:\n",
    "            y1 = y0 + win_size\n",
    "            y_mask = (y >= y0) & (y < y1)\n",
    "            idx = np.where(x_mask & y_mask)[0]\n",
    "            if idx.size == 0:\n",
    "                continue\n",
    "            # bucket by class for this window\n",
    "            by_class = {}\n",
    "            for c in np.unique(classes[idx]):\n",
    "                by_class[c] = idx[classes[idx] == c]\n",
    "            win_index[(float(x0), float(y0))] = by_class\n",
    "    return win_index\n",
    "\n",
    "\n",
    "def compute_ligand_exposure(\n",
    "    allexp: pd.DataFrame,\n",
    "    X_ligands: pd.DataFrame,\n",
    "    results_df: pd.DataFrame,\n",
    "    islands: list,\n",
    "    aux: pd.DataFrame,\n",
    "    theta: float = 0.5,              # coverage threshold to deem A↔B island-sharing\n",
    "    k_support: int = 3,              # min #covering windows for the receiver cell\n",
    "    mode: str = \"mean\",              # \"mean\" or \"kde\"\n",
    "    sigma: float | None = None,      # KDE sigma in coordinate units; default = win_size/3\n",
    "    class_col: str = \"class\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Step 2 — Compute ligand exposure via colocalization islands.\n",
    "\n",
    "    For each receiver cell i:\n",
    "      1) Find partner types B with aux.loc[i, f\"coverage_{B}\"] >= theta AND support_windows >= k_support\n",
    "      2) Collect neighbors = union of cells of those B-types that fall in ANY sliding window covering i\n",
    "         AND that window is an island window for the (A_i, B) pair.\n",
    "      3) Exposure for each ligand gene g = average (mode=\"mean\") or KDE-weighted average (mode=\"kde\")\n",
    "         of X_ligands[g] across the neighbor cells.\n",
    "\n",
    "    Returns:\n",
    "      X_exposure: DataFrame (cells × ligand genes), index aligned with allexp / X_ligands\n",
    "    \"\"\"\n",
    "    # --- grid and island caches ---\n",
    "    xs, ys, win_size = _extract_grid_from_results(results_df)\n",
    "    pair_to_windows, _ = _island_windows_by_pair(islands)  # {(A,B): set[(x0,y0)], ...}\n",
    "\n",
    "    # pre-index cells per (x0,y0) window and class for quick union queries\n",
    "    win_index = _preindex_window_cells(allexp, xs, ys, win_size, class_col=class_col)\n",
    "\n",
    "    # map from aux columns \"coverage_B\" → B\n",
    "    coverage_cols = {\n",
    "        c.replace(\"coverage_\", \"\"): c\n",
    "        for c in aux.columns if c.startswith(\"coverage_\")\n",
    "    }\n",
    "\n",
    "    # output matrix\n",
    "    X_exposure = pd.DataFrame(0.0, index=allexp.index, columns=X_ligands.columns)\n",
    "\n",
    "    # coords & class arrays for KDE mode\n",
    "    coord = allexp[[\"x\", \"y\"]].to_numpy(float)\n",
    "    classes = allexp[class_col].astype(str).to_numpy()\n",
    "\n",
    "    # default sigma ~ window size / 3 (smooth inside an island window)\n",
    "    if sigma is None:\n",
    "        sigma = win_size / 3.0 if np.isfinite(win_size) and win_size > 0 else 1.0\n",
    "    inv2sig2 = 1.0 / (2.0 * (sigma ** 2))\n",
    "\n",
    "    # --- main loop over receiver cells ---\n",
    "    for i in range(len(allexp)):\n",
    "        A = classes[i]\n",
    "        # support (how many windows cover this cell)\n",
    "        covered_i = _covering_windows(coord[i,0], coord[i,1], xs, ys, win_size)\n",
    "        support = len(covered_i)\n",
    "        if support < k_support:\n",
    "            continue  # exposure stays 0\n",
    "\n",
    "        # partner types B that share island with i (per aux coverage threshold)\n",
    "        eligible_B = [B for B, ccol in coverage_cols.items()\n",
    "                      if B != A and aux.iloc[i][ccol] >= theta]\n",
    "\n",
    "        if not eligible_B:\n",
    "            continue\n",
    "\n",
    "        # collect neighbors: union over B and over windows that cover i,\n",
    "        # intersected with island windows for (A,B)\n",
    "        nbr_idx = set()\n",
    "        covered_set = set(covered_i)\n",
    "        for B in eligible_B:\n",
    "            key = (A, B) if (A, B) in pair_to_windows else (B, A)\n",
    "            if key not in pair_to_windows:\n",
    "                continue\n",
    "            # windows that both cover i AND are island windows for (A,B)\n",
    "            isl_windows = pair_to_windows[key] & covered_set\n",
    "            if not isl_windows:\n",
    "                continue\n",
    "            # gather all B-type cells inside those windows\n",
    "            for w in isl_windows:\n",
    "                by_class = win_index.get(w)\n",
    "                if not by_class:\n",
    "                    continue\n",
    "                idxB = by_class.get(B)\n",
    "                if idxB is not None and idxB.size:\n",
    "                    nbr_idx.update(idxB.tolist())\n",
    "\n",
    "        if not nbr_idx:\n",
    "            continue\n",
    "\n",
    "        nbr_idx = np.fromiter(nbr_idx, dtype=int, count=len(nbr_idx))\n",
    "\n",
    "        if mode == \"mean\":\n",
    "            # simple average across eligible neighbors\n",
    "            X_exposure.iloc[i, :] = X_ligands.iloc[nbr_idx, :].mean(axis=0).fillna(0.0).to_numpy()\n",
    "\n",
    "        elif mode == \"kde\":\n",
    "            # Gaussian weights by distance to receiver cell i (within chosen sigma)\n",
    "            d2 = np.sum((coord[nbr_idx] - coord[i])**2, axis=1)  # squared distances\n",
    "            w = np.exp(-d2 * inv2sig2)\n",
    "            w_sum = np.sum(w)\n",
    "            if w_sum <= 0:\n",
    "                continue\n",
    "            w = w / w_sum\n",
    "            # weighted average per ligand gene\n",
    "            # (vectorized: neighbors × genes  @ weights)\n",
    "            X_exposure.iloc[i, :] = np.dot(w, X_ligands.iloc[nbr_idx, :].to_numpy())\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'mean' or 'kde'\")\n",
    "\n",
    "    return X_exposure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa935833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a threshold consistent with your binary encoding step\n",
    "theta = 0.5\n",
    "k_support = 3\n",
    "\n",
    "# Mean-based exposure (fast, good baseline)\n",
    "X_exposure_mean = compute_ligand_exposure(\n",
    "    allexp=allexp,\n",
    "    X_ligands=X_ligands,\n",
    "    results_df=results_df,     # from your (weighted) PCC stage\n",
    "    islands=islands,           # from find_islands_for_all_pairs(...)\n",
    "    aux=aux,                   # has coverage_* and support_windows\n",
    "    theta=theta,\n",
    "    k_support=k_support,\n",
    "    mode=\"mean\"\n",
    ")\n",
    "\n",
    "# KDE-weighted exposure (distance-weighted inside island windows)\n",
    "# X_exposure_kde = compute_ligand_exposure(\n",
    "#     allexp=allexp,\n",
    "#     X_ligands=X_ligands,\n",
    "#     results_df=results_df,\n",
    "#     islands=islands,\n",
    "#     aux=aux,\n",
    "#     theta=theta,\n",
    "#     k_support=k_support,\n",
    "#     mode=\"kde\",\n",
    "#     sigma=None   # defaults to win_size/3\n",
    "# )\n",
    "\n",
    "# Quick QC:\n",
    "print(\"Nonzero exposure frac (mean):\",\n",
    "      (X_exposure_mean.values > 0).mean())\n",
    "# print(\"Nonzero exposure frac (kde):\",\n",
    "#       (X_exposure_kde.values > 0).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24bbb38",
   "metadata": {},
   "source": [
    "## LR Interaction Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba1175e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_lr_interaction_features(\n",
    "    X_receptors: pd.DataFrame,\n",
    "    X_exposure: pd.DataFrame,          # ligand *exposure* matrix (not raw ligand expr)\n",
    "    lr_pairs_kept: pd.DataFrame,       # from Step 1 (already filtered to present genes)\n",
    "    ligand_col: str = \"ligand_symbol\",\n",
    "    receptor_col: str = \"receptor_symbol\",\n",
    "    method: str = \"product\",           # \"product\" or \"min\"\n",
    "    suffix: str = \"\"                   # optional suffix for column names, e.g., \"_prod\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Step 3 — Build LR interaction features.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    X_receptors : DataFrame (cells × unique receptor genes)\n",
    "    X_exposure  : DataFrame (cells × unique ligand genes)  [ligand *exposure*]\n",
    "    lr_pairs_kept : DataFrame containing at least [ligand_col, receptor_col]\n",
    "                    and (optionally) integer columns 'ligand_idx','receptor_idx'\n",
    "                    that index into X_exposure / X_receptors respectively.\n",
    "    method : \"product\" (R * Lexp) or \"min\" (min(R, Lexp))\n",
    "    suffix : optional string appended to interaction column names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_LR  : DataFrame (cells × n_pairs), interaction per LR pair\n",
    "    X_aux : DataFrame with receptor-only and ligand-exposure-only cols used\n",
    "    meta  : dict with bookkeeping (pair->indices, method)\n",
    "    \"\"\"\n",
    "    # --- Resolve indices for each pair into the receptor/exposure matrices ---\n",
    "    # Prefer the precomputed indices from Step 1 if available (fast, robust).\n",
    "    have_idx = {\"ligand_idx\" in lr_pairs_kept.columns,\n",
    "                \"receptor_idx\" in lr_pairs_kept.columns}\n",
    "    have_idx = all(have_idx)\n",
    "\n",
    "    # Maps for name→position (fallback if indices absent)\n",
    "    rec_pos = {g: i for i, g in enumerate(X_receptors.columns)}\n",
    "    lig_pos = {g: i for i, g in enumerate(X_exposure.columns)}\n",
    "\n",
    "    # Build ordered lists of positions and names aligned to lr_pairs_kept rows\n",
    "    lig_names = []\n",
    "    rec_names = []\n",
    "    lig_idx = []\n",
    "    rec_idx = []\n",
    "\n",
    "    for _, row in lr_pairs_kept.iterrows():\n",
    "        L = row[ligand_col]\n",
    "        R = row[receptor_col]\n",
    "\n",
    "        # If indices were carried from Step 1, use them; else resolve by column name.\n",
    "        if have_idx:\n",
    "            li = int(row[\"ligand_idx\"])\n",
    "            ri = int(row[\"receptor_idx\"])\n",
    "            # sanity: ensure columns still match names\n",
    "            assert X_exposure.columns[li] == L, f\"Ligand index/name mismatch: {L}\"\n",
    "            assert X_receptors.columns[ri] == R, f\"Receptor index/name mismatch: {R}\"\n",
    "        else:\n",
    "            # Fallback resolution via column names\n",
    "            if L not in lig_pos or R not in rec_pos:\n",
    "                # Skip pairs whose genes are missing (should be rare after Step 1 filtering)\n",
    "                continue\n",
    "            li = lig_pos[L]\n",
    "            ri = rec_pos[R]\n",
    "\n",
    "        lig_names.append(L)\n",
    "        rec_names.append(R)\n",
    "        lig_idx.append(li)\n",
    "        rec_idx.append(ri)\n",
    "\n",
    "    n_pairs = len(lig_idx)\n",
    "    if n_pairs == 0:\n",
    "        raise ValueError(\"No LR pairs could be aligned to X_receptors/X_exposure.\")\n",
    "\n",
    "    # --- Pull the arrays we need (vectorized over cells × pairs) ---\n",
    "    # R: receptor expression for each pair\n",
    "    R = X_receptors.iloc[:, rec_idx].to_numpy(dtype=float)   # shape: (cells, n_pairs)\n",
    "    # Lexp: ligand *exposure* for each pair\n",
    "    Lexp = X_exposure.iloc[:, lig_idx].to_numpy(dtype=float) # shape: (cells, n_pairs)\n",
    "\n",
    "    # --- Interaction function ---\n",
    "    if method.lower() == \"product\":\n",
    "        S = R * Lexp\n",
    "        method_used = \"product\"\n",
    "        suf = suffix or \"_prod\"\n",
    "    elif method.lower() == \"min\":\n",
    "        S = np.minimum(R, Lexp)\n",
    "        method_used = \"min\"\n",
    "        suf = suffix or \"_min\"\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'product' or 'min'\")\n",
    "\n",
    "    # --- Build nice column names like 'LIGAND|RECEPTOR_prod' ---\n",
    "    lr_labels = [f\"{L}|{R}{suf}\" for L, R in zip(lig_names, rec_names)]\n",
    "    X_LR = pd.DataFrame(S, index=X_receptors.index, columns=lr_labels)\n",
    "\n",
    "    # --- Auxiliary matrices used in these interactions (optional but useful) ---\n",
    "    # receptor-only features (just those receptors that appear in pairs, once)\n",
    "    # ligand-only (exposure) features (just those ligands that appear in pairs, once)\n",
    "    # We keep column order matched to the *first* time a gene appears in lr_pairs_kept.\n",
    "    uniq_rec_ordered = list(dict.fromkeys(rec_names))\n",
    "    uniq_lig_ordered = list(dict.fromkeys(lig_names))\n",
    "\n",
    "    X_rec_used  = X_receptors.loc[:, uniq_rec_ordered].copy()\n",
    "    X_lig_used  = X_exposure.loc[:, uniq_lig_ordered].copy()\n",
    "    # Prefix to make it explicit in modeling design matrices\n",
    "    X_rec_used.columns = [f\"R__{c}\"     for c in X_rec_used.columns]\n",
    "    X_lig_used.columns = [f\"Lexp__{c}\"  for c in X_lig_used.columns]\n",
    "\n",
    "    X_aux = pd.concat([X_rec_used, X_lig_used], axis=1)\n",
    "\n",
    "    meta = {\n",
    "        \"method\": method_used,\n",
    "        \"pairs\": list(zip(lig_names, rec_names)),\n",
    "        \"ligand_indices\": lig_idx,\n",
    "        \"receptor_indices\": rec_idx,\n",
    "        \"n_pairs\": n_pairs\n",
    "    }\n",
    "    return X_LR, X_aux, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c341e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Example usage\n",
    "# ---------------------------\n",
    "X_LR_prod, X_aux, meta = build_lr_interaction_features(\n",
    "    X_receptors=X_receptors,\n",
    "    X_exposure=X_exposure_mean,         # or X_exposure_kde\n",
    "    lr_pairs_kept=lr_pairs_kept,\n",
    "    ligand_col=\"ligand_symbol\",\n",
    "    receptor_col=\"receptor_symbol\",\n",
    "    method=\"product\"\n",
    ")\n",
    "\n",
    "# X_LR_min, X_aux_min, meta_min = build_lr_interaction_features(..., method=\"min\")\n",
    "\n",
    "# Quick sanity checks:\n",
    "# 1) No NaNs, non-negativity if inputs are non-negative\n",
    "assert np.isfinite(X_LR_prod.to_numpy()).all()\n",
    "# 2) Column count equals number of kept pairs\n",
    "assert X_LR_prod.shape[1] == meta[\"n_pairs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9b3bd",
   "metadata": {},
   "source": [
    "## Gene Filtering\n",
    "\n",
    "Remove ligands, receptors and bottom 5% genes in terms of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9f526d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ligands = LR_pairs['ligand_genesymbol'].unique()\n",
    "receptors = LR_pairs['target_genesymbol'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf4334c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = aux.reset_index().rename(columns={'index': 'cell_label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0de62ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should already have these from Steps 1–3 & island encoding:\n",
    "# - allexp: DataFrame (cells × [meta + genes]); index = cell ids; columns include 'x','y','class','cell_label' etc.\n",
    "# - X_receptors: DataFrame (cells × receptors)              # Step 1\n",
    "# - X_exposure:  DataFrame (cells × ligands)                # Step 2\n",
    "# - X_LR:        DataFrame (cells × LR-pair interactions)   # Step 3\n",
    "# - aux:         DataFrame with 'support_windows' and columns starting with 'coverage_'\n",
    "# - lr_pairs_kept: DataFrame with 'ligand_symbol', 'receptor_symbol'\n",
    "\n",
    "# Sanity:\n",
    "for name in [\"allexp\", \"X_receptors\", \"X_exposure_mean\", \"X_LR_prod\", \"aux\", \"lr_pairs_kept\"]:\n",
    "    assert name in globals(), f\"Missing variable: {name}\"\n",
    "\n",
    "# Align row order across all matrices by allexp:\n",
    "idx = allexp.index\n",
    "X_receptors = X_receptors.loc[idx]\n",
    "X_exposure  = X_exposure_mean.loc[idx]\n",
    "X_LR        = X_LR_prod.loc[idx]\n",
    "aux         = aux.loc[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d24518b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "\n",
    "# CONFIG\n",
    "META_COLS          = [\"x\",\"y\",\"class\",\"cell_label\"]  # adjust if you have different meta cols\n",
    "DROP_LIGANDS       = True     # recommended to avoid leakage; set False if you want to include ligands as targets\n",
    "MIN_DETECT_FRAC    = 0.01     # drop targets detected in <1% of cells\n",
    "MIN_VAR_QUANT      = 0.10     # drop bottom 10% variance targets\n",
    "DROP_TECHNICALS    = True     # drop mito/ribo/hb-like genes if present\n",
    "COV_PREFIX         = \"coverage_\"\n",
    "\n",
    "def get_gene_matrix(allexp: pd.DataFrame, meta_cols=META_COLS) -> pd.DataFrame:\n",
    "    \"\"\"Return cells × genes numeric matrix by dropping meta columns.\"\"\"\n",
    "    meta_cols = [c for c in meta_cols if c in allexp.columns]\n",
    "    gene_cols = [c for c in allexp.columns if c not in meta_cols]\n",
    "    expr = allexp.loc[:, gene_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "    return expr\n",
    "\n",
    "def zscore_df(df: pd.DataFrame):\n",
    "    \"\"\"Z-score columns; returns (Z, scaler).\"\"\"\n",
    "    sc = StandardScaler(with_mean=True, with_std=True)\n",
    "    Z = sc.fit_transform(df.values)\n",
    "    return pd.DataFrame(Z, index=df.index, columns=df.columns), sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c23391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Full cells×genes matrix\n",
    "expr_all = get_gene_matrix(allexp_sub, meta_cols=META_COLS)\n",
    "\n",
    "# 2) Receptor & ligand sets from your LR list\n",
    "receptors = sorted(set(lr_pairs_kept[\"receptor_symbol\"]))\n",
    "ligands   = sorted(set(lr_pairs_kept[\"ligand_symbol\"]))\n",
    "\n",
    "# 3) Build the drop list for targets\n",
    "drop = set(expr_all.columns).intersection(receptors)\n",
    "if DROP_LIGANDS:\n",
    "    drop |= set(expr_all.columns).intersection(ligands)\n",
    "\n",
    "# 4) Filter low detection / low variance\n",
    "det_frac = (expr_all > 0).mean(axis=0)                            # fraction of cells with nonzero expression\n",
    "var_g    = expr_all.var(axis=0)\n",
    "low_det  = det_frac[det_frac < MIN_DETECT_FRAC].index\n",
    "low_var  = var_g[var_g < var_g.quantile(MIN_VAR_QUANT)].index\n",
    "drop |= set(low_det) | set(low_var)\n",
    "\n",
    "# 5) Optional: remove likely technical genes if present among your 500\n",
    "if DROP_TECHNICALS:\n",
    "    tech_re = re.compile(r\"^(MT-|mt-|RPL|RPS|HBA|HBB)\")\n",
    "    tech = [g for g in expr_all.columns if tech_re.match(g)]\n",
    "    drop |= set(tech)\n",
    "\n",
    "# 6) Final targets\n",
    "target_genes = [g for g in expr_all.columns if g not in drop]\n",
    "Y_targets = expr_all.loc[idx, target_genes].copy()\n",
    "\n",
    "print(f\"Targets kept: {Y_targets.shape[1]} genes (from {expr_all.shape[1]} total).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5e4b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use every coverage_* column (broader approach)\n",
    "coverage_cols_all = [c for c in aux.columns if c.startswith(COV_PREFIX)]\n",
    "assert len(coverage_cols_all) > 0, \"No coverage_* columns in aux.\"\n",
    "\n",
    "X_cov = aux[coverage_cols_all].copy()\n",
    "# Optional rename to cleaner names for modeling; keep originals if you prefer\n",
    "X_cov.columns = [f\"cov::{c.replace(COV_PREFIX,'')}\" for c in coverage_cols_all]\n",
    "\n",
    "# Sample weights from window support (use later in training)\n",
    "sample_weight = aux[\"support_windows\"].clip(lower=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6fda5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score each block independently (keep scalers to transform CV folds later)\n",
    "X_receptors_z, sc_R = zscore_df(X_receptors)\n",
    "X_exposure_z,  sc_E = zscore_df(X_exposure)\n",
    "X_LR_z,        sc_I = zscore_df(X_LR)\n",
    "X_cov_z,       sc_C = zscore_df(X_cov)\n",
    "\n",
    "# Concatenate in fixed order\n",
    "X = pd.concat([X_receptors_z, X_exposure_z, X_LR_z, X_cov_z], axis=1)\n",
    "\n",
    "print(\"X shape:\", X.shape)                # (cells × features)\n",
    "print(\"Y_targets shape:\", Y_targets.shape) # (cells × target genes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "142e2122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic sanity\n",
    "import numpy as np\n",
    "assert np.isfinite(X.values).all(), \"Non-finite values in X\"\n",
    "assert np.isfinite(Y_targets.values).all(), \"Non-finite values in Y_targets\"\n",
    "\n",
    "# Sparsity snapshots (pre-zscore)\n",
    "print(\"Nonzero frac (receptors):\", (X_receptors.values != 0).mean())\n",
    "print(\"Nonzero frac (exposure) :\", (X_exposure.values  != 0).mean())\n",
    "print(\"Nonzero frac (LR)       :\", (X_LR.values        != 0).mean())\n",
    "print(\"Mean coverage (X_cov)   :\", X_cov.mean().mean())\n",
    "\n",
    "# Ready for Step 5: spatial block CV + multi-task Elastic Net / RF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062ef074",
   "metadata": {},
   "source": [
    "# Model Training (ElasticNet: X_rec, X_exp, X_LR, X_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f7a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "from sklearn.metrics import r2_score\n",
    "from itertools import product\n",
    "\n",
    "# ---- Repro & CV/Test config ----\n",
    "SEED = 42\n",
    "N_GROUPS = 10          # number of spatial groups to form from (x,y)\n",
    "TEST_FRACTION = 0.2    # ~20% groups as final test set\n",
    "N_SPLITS = 5           # GroupKFold folds on the dev set\n",
    "\n",
    "# ---- Elastic Net search grid ----\n",
    "ALPHAS = np.logspace(-4, 1, 8)      # 1e-4 ... 10\n",
    "L1S    = [0.1, 0.5, 0.9]            # l1_ratio\n",
    "\n",
    "# ---- Metric aggregation choice ----\n",
    "AGG = \"mean\"   # \"mean\" or \"median\" R^2 across targets\n",
    "USE_WEIGHTS_IN_SCORING = True  # use support_windows as weights in metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddfeb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x,y) → spatial groups\n",
    "coords = allexp[[\"x\",\"y\"]].to_numpy(dtype=float)\n",
    "\n",
    "km = KMeans(n_clusters=N_GROUPS, n_init=10, random_state=SEED)\n",
    "group_labels = km.fit_predict(coords)  # 0..N_GROUPS-1, one label per cell\n",
    "\n",
    "# Decide test groups (≈ TEST_FRACTION of groups)\n",
    "rng = np.random.default_rng(SEED)\n",
    "unique_groups = np.arange(N_GROUPS)\n",
    "n_test_groups = max(1, int(round(TEST_FRACTION * N_GROUPS)))\n",
    "test_groups = rng.choice(unique_groups, size=n_test_groups, replace=False)\n",
    "\n",
    "is_test = np.isin(group_labels, test_groups)\n",
    "is_dev  = ~is_test\n",
    "\n",
    "print(\"Groups:\", N_GROUPS, \"| Test groups:\", sorted(test_groups.tolist()))\n",
    "print(\"Dev cells:\", is_dev.sum(), \" Test cells:\", is_test.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ebf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform_split(X, Y, train_idx, val_idx):\n",
    "    \"\"\"\n",
    "    Fit scalers on TRAIN only; transform train/val for both X and Y.\n",
    "    Returns: Xtr, Xva, Ytr, Yva, scalers (sx, sy)\n",
    "    \"\"\"\n",
    "    sx = StandardScaler(with_mean=True, with_std=True)\n",
    "    sy = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "    Xtr = sx.fit_transform(X[train_idx])\n",
    "    Xva = sx.transform(X[val_idx])\n",
    "\n",
    "    Ytr = sy.fit_transform(Y[train_idx])\n",
    "    Yva = sy.transform(Y[val_idx])\n",
    "\n",
    "    return Xtr, Xva, Ytr, Yva, sx, sy\n",
    "\n",
    "def weighted_r2_per_target(y_true, y_pred, sample_weight=None):\n",
    "    \"\"\"\n",
    "    R² per target column (multioutput). sklearn's r2_score with multioutput=None\n",
    "    for each column; supports sample weights if provided.\n",
    "    \"\"\"\n",
    "    T = y_true.shape[1]\n",
    "    r2s = np.empty(T, dtype=float)\n",
    "    for t in range(T):\n",
    "        r2s[t] = r2_score(y_true[:, t], y_pred[:, t], sample_weight=sample_weight)\n",
    "    return r2s\n",
    "\n",
    "def aggregate_scores(r2_vec, agg=\"mean\"):\n",
    "    return float(np.nanmean(r2_vec)) if agg == \"mean\" else float(np.nanmedian(r2_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice dev/test once\n",
    "X_all = X.to_numpy(dtype=float)\n",
    "Y_all = Y_targets.to_numpy(dtype=float)\n",
    "\n",
    "groups_dev = group_labels[is_dev]\n",
    "X_dev, Y_dev = X_all[is_dev], Y_all[is_dev]\n",
    "weights_all = aux[\"support_windows\"].to_numpy()\n",
    "w_dev = weights_all[is_dev] if USE_WEIGHTS_IN_SCORING else None\n",
    "\n",
    "gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "\n",
    "cv_results = []  # collect dicts for a summary table\n",
    "\n",
    "for alpha, l1 in product(ALPHAS, L1S):\n",
    "    fold_scores = []\n",
    "    per_target_scores = []  # optional: store mean per-target R² across folds too\n",
    "\n",
    "    for tr_idx, va_idx in gkf.split(X_dev, groups=groups_dev):\n",
    "        # Train/val split indexes relative to DEV subset\n",
    "        Xtr, Xva, Ytr, Yva, sx, sy = fit_transform_split(X_dev, Y_dev, tr_idx, va_idx)\n",
    "\n",
    "        # Model\n",
    "        model = MultiTaskElasticNet(\n",
    "            alpha=alpha,\n",
    "            l1_ratio=l1,\n",
    "            fit_intercept=False,   # we already standardized\n",
    "            max_iter=5000,\n",
    "            random_state=SEED,\n",
    "            selection=\"cyclic\"\n",
    "        )\n",
    "        model.fit(Xtr, Ytr)\n",
    "\n",
    "        # Predict and score\n",
    "        Yhat = model.predict(Xva)\n",
    "        sw = w_dev[va_idx] if w_dev is not None else None\n",
    "        r2_t = weighted_r2_per_target(Yva, Yhat, sample_weight=sw)\n",
    "        fold_scores.append(aggregate_scores(r2_t, AGG))\n",
    "        per_target_scores.append(r2_t)\n",
    "\n",
    "    cv_results.append({\n",
    "        \"alpha\": alpha,\n",
    "        \"l1_ratio\": l1,\n",
    "        \"cv_score\": float(np.mean(fold_scores)),\n",
    "        \"cv_score_std\": float(np.std(fold_scores)),\n",
    "        \"per_target_mean\": float(np.mean(np.vstack(per_target_scores), axis=0).mean()),\n",
    "    })\n",
    "\n",
    "# Pick best by cv_score\n",
    "cv_df = pd.DataFrame(cv_results).sort_values([\"cv_score\", \"per_target_mean\"], ascending=[False, False]).reset_index(drop=True)\n",
    "best = cv_df.iloc[0].to_dict()\n",
    "best_alpha, best_l1 = float(best[\"alpha\"]), float(best[\"l1_ratio\"])\n",
    "\n",
    "print(\"Best hyperparams → alpha=%.4g, l1_ratio=%.2f | CV mean %s R²=%.4f (± %.4f)\" %\n",
    "      (best_alpha, best_l1, AGG, best[\"cv_score\"], best[\"cv_score_std\"]))\n",
    "\n",
    "cv_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac81aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa353a",
   "metadata": {},
   "source": [
    "# Model Training (XGBoost: X_rec, X_exp, X_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5632ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from itertools import product\n",
    "\n",
    "# ---- Core choices ----\n",
    "SEED = 42\n",
    "N_GROUPS = 8             # spatial groups per receiver type (adjust 5–12)\n",
    "TEST_FRACTION = 0.2      # fraction of groups held-out as final test\n",
    "N_SPLITS = 5             # GroupKFold on the dev set\n",
    "IGNORE_ZERO_COV = True   # << drop cells whose coverage is 0 for ALL sender types\n",
    "USE_SAMPLE_WEIGHTS = True  # use aux['support_windows'] as weights\n",
    "\n",
    "# ---- Targets selection (to keep runtime sane) ----\n",
    "TARGET_LIMIT = 300       # top-variance targets per receiver type (None = all)\n",
    "\n",
    "# ---- XGBoost search space (small but effective) ----\n",
    "ALPHAS_REG_L2 = [1, 5, 10]         # reg_lambda\n",
    "ALPHAS_REG_L1 = [0, 1]             # reg_alpha\n",
    "MAX_DEPTHS    = [4, 6]\n",
    "LEARNING_RATES= [0.03, 0.1]\n",
    "SUBSAMPLE     = [0.8, 1.0]\n",
    "COLSAMPLE     = [0.8, 1.0]\n",
    "\n",
    "N_ESTIMATORS  = 2000\n",
    "EARLY_STOP    = 100\n",
    "TREE_METHOD   = \"hist\"  # 'hist' is fast and scalable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef74f2b",
   "metadata": {},
   "source": [
    "## Minimal Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57aec4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast, single-setting run (no grid, no CV)\n",
    "SEED = 42\n",
    "N_GROUPS = 6            # fewer groups = faster\n",
    "TEST_FRACTION = 0.2\n",
    "IGNORE_ZERO_COV = True  # drop cells with all-zero coverage\n",
    "USE_SAMPLE_WEIGHTS = True\n",
    "TARGET_LIMIT = 150      # fewer targets for speed (set None for all)\n",
    "EARLY_STOP = 80         # patience\n",
    "EVAL_HOLDOUT = 0.1      # 10% of dev for early stopping\n",
    "# Fixed XGBoost params (sane defaults)\n",
    "XGB_PARAMS = dict(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    n_estimators=2000,\n",
    "    objective=\"reg:squarederror\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=SEED,\n",
    "    n_jobs=0,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=EARLY_STOP\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e58ea4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def make_groups(xy, k, seed=SEED):\n",
    "    k = int(min(max(k, 3), len(xy)))\n",
    "    return KMeans(n_clusters=k, n_init=10, random_state=seed).fit_predict(xy)\n",
    "\n",
    "def split_dev_test_by_groups(groups, frac=0.2, seed=SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    ug = np.unique(groups)\n",
    "    n_test = max(1, int(round(frac * len(ug))))\n",
    "    test_g = rng.choice(ug, size=n_test, replace=False)\n",
    "    is_test = np.isin(groups, test_g)\n",
    "    return ~is_test, is_test, test_g\n",
    "\n",
    "def r2_weighted(y_true, y_pred, w=None):\n",
    "    if w is None:\n",
    "        return r2_score(y_true, y_pred)\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred); w = np.asarray(w)\n",
    "    w = w / (w.sum() + 1e-12)\n",
    "    y_bar = np.sum(w * y_true)\n",
    "    sse = np.sum(w * (y_true - y_pred)**2)\n",
    "    sst = np.sum(w * (y_true - y_bar)**2)\n",
    "    return 1.0 - (sse / (sst + 1e-12))\n",
    "\n",
    "def quick_train_xgb_per_receiver(receiver_type: str):\n",
    "    # Subset rows for this receiver type\n",
    "    idx = allexp.index[allexp[\"class\"] == receiver_type]\n",
    "    X_block = pd.concat([X_receptors.loc[idx], X_exposure.loc[idx], X_cov.loc[idx]], axis=1)\n",
    "    Y_block = Y_targets.loc[idx]\n",
    "    xy = allexp.loc[idx, [\"x\",\"y\"]].to_numpy(float)\n",
    "    w_all = aux.loc[idx, \"support_windows\"].to_numpy() if USE_SAMPLE_WEIGHTS else None\n",
    "\n",
    "    # Optional: drop cells with all-zero coverage\n",
    "    if IGNORE_ZERO_COV:\n",
    "        keep = (X_cov.loc[idx].sum(axis=1) > 0).to_numpy()\n",
    "        X_block = X_block.loc[keep]\n",
    "        Y_block = Y_block.loc[keep]\n",
    "        xy = xy[keep]\n",
    "        if w_all is not None: w_all = w_all[keep]\n",
    "\n",
    "    # Limit targets for speed\n",
    "    if TARGET_LIMIT is not None:\n",
    "        top = Y_block.var(axis=0).sort_values(ascending=False).index[:TARGET_LIMIT]\n",
    "        Y_block = Y_block.loc[:, top]\n",
    "\n",
    "    # Spatial groups → dev/test split\n",
    "    groups = make_groups(xy, N_GROUPS, seed=SEED)\n",
    "    is_dev, is_test, test_groups = split_dev_test_by_groups(groups, frac=TEST_FRACTION, seed=SEED)\n",
    "    print(X_block.shape)\n",
    "    X_dev = X_block.to_numpy()[is_dev]; X_test = X_block.to_numpy()[is_test]\n",
    "    Y_dev = Y_block.to_numpy()[is_dev]; Y_test = Y_block.to_numpy()[is_test]\n",
    "    w_dev = w_all[is_dev] if w_all is not None else None\n",
    "    w_test = w_all[is_test] if w_all is not None else None\n",
    "\n",
    "    # Internal eval split from dev for early stopping (no CV)\n",
    "    n_dev = X_dev.shape[0]\n",
    "    n_eval = max(1, int(EVAL_HOLDOUT * n_dev))\n",
    "    tr_mask = np.ones(n_dev, dtype=bool); tr_mask[-n_eval:] = False\n",
    "    va_mask = ~tr_mask\n",
    "\n",
    "    results, models = [], {}\n",
    "    genes = Y_block.columns.tolist()\n",
    "    for i in tqdm(range(len(genes))):\n",
    "        gi = i + 1\n",
    "        gene = genes[i]\n",
    "        y_dev = Y_dev[:, gi-1]; y_test = Y_test[:, gi-1]\n",
    "        model = XGBRegressor(**XGB_PARAMS)\n",
    "        model.fit(\n",
    "            X_dev[tr_mask], y_dev[tr_mask],\n",
    "            sample_weight=(w_dev[tr_mask] if w_dev is not None else None),\n",
    "            eval_set=[(X_dev[va_mask], y_dev[va_mask])],\n",
    "            sample_weight_eval_set=[w_dev[va_mask]] if w_dev is not None else None,\n",
    "            verbose=False\n",
    "        )\n",
    "        yhat = model.predict(X_test)\n",
    "        r2t = r2_weighted(y_test, yhat, w_test)\n",
    "        results.append({\"receiver_type\": receiver_type,\n",
    "                        \"gene\": gene,\n",
    "                        \"test_r2\": float(r2t),\n",
    "                        \"best_iters\": int(getattr(model, \"best_iteration_\", None) or model.get_params()[\"n_estimators\"])})\n",
    "        models[gene] = model\n",
    "\n",
    "    summary = pd.DataFrame(results).sort_values(\"test_r2\", ascending=False).reset_index(drop=True)\n",
    "    return summary, models, test_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7dde9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "receiver_types = sorted(allexp[\"class\"].unique())\n",
    "\n",
    "all_summaries = []\n",
    "all_models = {}\n",
    "heldout = {}\n",
    "\n",
    "for rtype in receiver_types:\n",
    "    print(f\"=== {rtype} ===\")\n",
    "    try:\n",
    "        s, m, tg = quick_train_xgb_per_receiver(rtype)\n",
    "        all_summaries.append(s)\n",
    "        all_models[rtype] = m\n",
    "        heldout[rtype] = tg\n",
    "        print(s.head(5), \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing receiver type {rtype}: {e}\")\n",
    "\n",
    "summary_all = pd.concat(all_summaries, ignore_index=True)\n",
    "print(\"Ballpark — top genes by test R² across receiver types:\")\n",
    "display(summary_all.head(15))\n",
    "\n",
    "print(\"Median test R² per receiver type:\")\n",
    "display(summary_all.groupby(\"receiver_type\")[\"test_r2\"].median().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "158d7854",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(all_summaries, ignore_index=True)\n",
    "\n",
    "# Step 2: Select only the required columns\n",
    "r2_summary = combined_df[['receiver_type', 'gene', 'test_r2']]\n",
    "\n",
    "# Step 3: Save to CSV\n",
    "r2_summary.to_csv('../data/r2_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1fa4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best models\n",
    "import joblib\n",
    "joblib.dump(all_models, \"xgb_models_per_receiver.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb61362",
   "metadata": {},
   "source": [
    "## Main Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected to already exist from your previous steps:\n",
    "# allexp, X_receptors, X_exposure, X_cov, aux, Y_targets\n",
    "for name in [\"allexp\", \"X_receptors\", \"X_exposure_mean\", \"X_cov\", \"aux\"]:\n",
    "    assert name in globals(), f\"Missing variable: {name}\"\n",
    "\n",
    "# If Y_targets was not created earlier, fall back to all non-receptor/non-ligand genes\n",
    "if \"Y_targets\" not in globals():\n",
    "    assert \"lr_pairs_kept\" in globals(), \"Need lr_pairs_kept to derive non-R/L targets.\"\n",
    "    META_COLS = [\"x\",\"y\",\"class\",\"cell_label\"]\n",
    "    expr_all = allexp.drop(columns=[c for c in META_COLS if c in allexp.columns], errors=\"ignore\")\n",
    "    receptors = set(lr_pairs_kept[\"receptor_symbol\"])\n",
    "    ligands   = set(lr_pairs_kept[\"ligand_symbol\"])\n",
    "    non_RL = [g for g in expr_all.columns if g not in receptors | ligands]\n",
    "    Y_targets = expr_all.loc[:, non_RL].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "def make_spatial_groups(xy, n_groups, seed=SEED):\n",
    "    n_groups = int(min(max(n_groups, 3), len(xy)))  # at least 3, at most n_samples\n",
    "    km = KMeans(n_clusters=n_groups, n_init=10, random_state=seed)\n",
    "    return km.fit_predict(xy)\n",
    "\n",
    "def split_dev_test_by_groups(groups, test_fraction=0.2, seed=SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    ug = np.unique(groups)\n",
    "    n_test = max(1, int(round(test_fraction * len(ug))))\n",
    "    test_g = rng.choice(ug, size=n_test, replace=False)\n",
    "    is_test = np.isin(groups, test_g)\n",
    "    return ~is_test, is_test, test_g\n",
    "\n",
    "def r2_weighted(y_true, y_pred, w=None):\n",
    "    if w is None:\n",
    "        return r2_score(y_true, y_pred)\n",
    "    # Weighted R²: 1 - SSE/SST with weights\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred); w = np.asarray(w)\n",
    "    w = w / (w.sum() + 1e-12)\n",
    "    y_bar = np.sum(w * y_true)\n",
    "    sse = np.sum(w * (y_true - y_pred)**2)\n",
    "    sst = np.sum(w * (y_true - y_bar)**2)\n",
    "    return 1.0 - (sse / (sst + 1e-12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_xgb_for_receiver(receiver_type: str):\n",
    "    \"\"\"\n",
    "    Trains XGBoost models for each target gene within a receiver cell type.\n",
    "    Uses spatial GroupKFold (groups via KMeans on x,y within this subset).\n",
    "    Inputs: X = [X_receptors, X_exposure, X_cov]; skips X_LR.\n",
    "    Optionally drops rows with all-zero coverage when IGNORE_ZERO_COV=True.\n",
    "    Returns:\n",
    "      summary_df: per-gene CV and test metrics + best params\n",
    "      models: dict gene -> fitted final dev model\n",
    "      test_groups: which spatial groups were held out\n",
    "    \"\"\"\n",
    "    # --- subset rows for this receiver type ---\n",
    "    idx_all = allexp.index[allexp[\"class\"] == receiver_type]\n",
    "    assert len(idx_all) > 0, f\"No cells for receiver type {receiver_type}\"\n",
    "\n",
    "    X_block = pd.concat([X_receptors.loc[idx_all],\n",
    "                         X_exposure.loc[idx_all],\n",
    "                         X_cov.loc[idx_all]], axis=1)\n",
    "    Y_block = Y_targets.loc[idx_all]\n",
    "    xy = allexp.loc[idx_all, [\"x\",\"y\"]].to_numpy(float)\n",
    "    w_all = aux.loc[idx_all, \"support_windows\"].to_numpy() if USE_SAMPLE_WEIGHTS else None\n",
    "\n",
    "    # Optionally drop cells with zero coverage across all sender types\n",
    "    if IGNORE_ZERO_COV:\n",
    "        cov_cols = X_cov.columns\n",
    "        mask_nonzero_cov = (X_cov.loc[idx_all, :].sum(axis=1) > 0).to_numpy()\n",
    "        X_block = X_block.loc[mask_nonzero_cov]\n",
    "        Y_block = Y_block.loc[mask_nonzero_cov]\n",
    "        xy      = xy[mask_nonzero_cov]\n",
    "        if w_all is not None:\n",
    "            w_all = w_all[mask_nonzero_cov]\n",
    "\n",
    "    # Pick targets (optionally limit to top-variance)\n",
    "    if TARGET_LIMIT is not None:\n",
    "        var = Y_block.var(axis=0)\n",
    "        top_genes = var.sort_values(ascending=False).index[:TARGET_LIMIT]\n",
    "        Y_block = Y_block.loc[:, top_genes]\n",
    "\n",
    "    # Build spatial groups within this receiver type\n",
    "    groups = make_spatial_groups(xy, n_groups=N_GROUPS, seed=SEED)\n",
    "\n",
    "    # Dev/Test split (by groups)\n",
    "    is_dev, is_test, test_groups = split_dev_test_by_groups(groups, test_fraction=TEST_FRACTION, seed=SEED)\n",
    "\n",
    "    X_dev, X_test = X_block.to_numpy()[is_dev], X_block.to_numpy()[is_test]\n",
    "    Y_dev, Y_test = Y_block.to_numpy()[is_dev], Y_block.to_numpy()[is_test]\n",
    "    groups_dev    = groups[is_dev]\n",
    "    w_dev = w_all[is_dev] if w_all is not None else None\n",
    "    w_test= w_all[is_test] if w_all is not None else None\n",
    "\n",
    "    # Grid of params to try\n",
    "    param_grid = list(product(MAX_DEPTHS, LEARNING_RATES, SUBSAMPLE, COLSAMPLE, ALPHAS_REG_L2, ALPHAS_REG_L1))\n",
    "\n",
    "    gkf = GroupKFold(n_splits=min(N_SPLITS, len(np.unique(groups_dev))))\n",
    "    genes = Y_block.columns.tolist()\n",
    "\n",
    "    results = []\n",
    "    models = {}\n",
    "\n",
    "    for gi, gene in enumerate(genes, 1):\n",
    "        y_dev = Y_dev[:, gi-1]\n",
    "        y_test= Y_test[:, gi-1]\n",
    "\n",
    "        best_score, best_params, best_n_rounds = -np.inf, None, None\n",
    "\n",
    "        # --- CV hyperparameter search ---\n",
    "        for (max_depth, eta, subs, colsub, reg_l2, reg_l1) in param_grid:\n",
    "            fold_scores, nrounds = [], []\n",
    "\n",
    "            for tr_idx, va_idx in gkf.split(X_dev, groups=groups_dev):\n",
    "                Xtr, Xva = X_dev[tr_idx], X_dev[va_idx]\n",
    "                ytr, yva = y_dev[tr_idx], y_dev[va_idx]\n",
    "                wtr = w_dev[tr_idx] if w_dev is not None else None\n",
    "                wva = w_dev[va_idx] if w_dev is not None else None\n",
    "\n",
    "                model = XGBRegressor(\n",
    "                    max_depth=max_depth,\n",
    "                    learning_rate=eta,\n",
    "                    subsample=subs,\n",
    "                    colsample_bytree=colsub,\n",
    "                    reg_lambda=reg_l2,\n",
    "                    reg_alpha=reg_l1,\n",
    "                    n_estimators=N_ESTIMATORS,\n",
    "                    objective=\"reg:squarederror\",\n",
    "                    tree_method=TREE_METHOD,\n",
    "                    random_state=SEED,\n",
    "                    n_jobs=0,\n",
    "                    eval_metric=\"rmse\",\n",
    "                    early_stopping_rounds=EARLY_STOP\n",
    "                )\n",
    "                model.fit(\n",
    "                    Xtr, ytr,\n",
    "                    sample_weight=wtr,\n",
    "                    eval_set=[(Xva, yva)],\n",
    "                    sample_weight_eval_set=[wva] if wva is not None else None,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                yhat = model.predict(Xva)\n",
    "                r2   = r2_weighted(yva, yhat, wva)\n",
    "                fold_scores.append(r2)\n",
    "                nrounds.append(model.best_iteration if model.best_iteration is not None else N_ESTIMATORS)\n",
    "\n",
    "            mean_score = float(np.mean(fold_scores))\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = dict(\n",
    "                    max_depth=max_depth, learning_rate=eta, subsample=subs,\n",
    "                    colsample_bytree=colsub, reg_lambda=reg_l2, reg_alpha=reg_l1\n",
    "                )\n",
    "                best_n_rounds = int(np.median(nrounds))\n",
    "\n",
    "        # --- Refit on ALL dev with best params ---\n",
    "        final = XGBRegressor(\n",
    "            **best_params,\n",
    "            n_estimators=max(best_n_rounds, 50),\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=TREE_METHOD,\n",
    "            random_state=SEED,\n",
    "            n_jobs=0,\n",
    "            eval_metric=\"rmse\",\n",
    "            early_stopping_rounds=min(EARLY_STOP, best_n_rounds//3 if best_n_rounds else EARLY_STOP)\n",
    "        )\n",
    "        # Use a small internal split from dev for early stopping without leaking test\n",
    "        # Here we reserve last 10% dev indices as eval (deterministic for reproducibility)\n",
    "        n_dev = X_dev.shape[0]\n",
    "        n_eval = max(1, int(0.1 * n_dev))\n",
    "        tr_mask = np.ones(n_dev, dtype=bool); tr_mask[-n_eval:] = False\n",
    "        va_mask = ~tr_mask\n",
    "\n",
    "        wtr = w_dev[tr_mask] if w_dev is not None else None\n",
    "        wva = w_dev[va_mask] if w_dev is not None else None\n",
    "\n",
    "        final.fit(\n",
    "            X_dev[tr_mask], y_dev[tr_mask],\n",
    "            sample_weight=wtr,\n",
    "            eval_set=[(X_dev[va_mask], y_dev[va_mask])],\n",
    "            sample_weight_eval_set=[wva] if wva is not None else None,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # Test evaluation\n",
    "        yhat_test = final.predict(X_test)\n",
    "        r2_test = r2_weighted(y_test, yhat_test, w_test)\n",
    "\n",
    "        # Store\n",
    "        results.append({\n",
    "            \"receiver_type\": receiver_type,\n",
    "            \"gene\": gene,\n",
    "            \"cv_mean_r2\": best_score,\n",
    "            \"test_r2\": float(r2_test),\n",
    "            \"best_n_estimators\": int(getattr(final, \"best_iteration\", None) or final.get_params()[\"n_estimators\"]),\n",
    "            **best_params\n",
    "        })\n",
    "        models[gene] = final\n",
    "\n",
    "    summary_df = pd.DataFrame(results).sort_values([\"test_r2\",\"cv_mean_r2\"], ascending=[False, False]).reset_index(drop=True)\n",
    "    return summary_df, models, test_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da66dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "receiver_types = sorted(allexp[\"class\"].unique())\n",
    "\n",
    "all_summaries = []\n",
    "all_models = {}\n",
    "heldout_groups = {}\n",
    "\n",
    "for rtype in tqdm(receiver_types):\n",
    "    print(f\"=== Training for receiver type: {rtype} ===\")\n",
    "    summary_df, models, test_groups = train_xgb_for_receiver(rtype)\n",
    "    all_summaries.append(summary_df)\n",
    "    all_models[rtype] = models\n",
    "    heldout_groups[rtype] = test_groups\n",
    "    print(summary_df.head(5), \"\\n\")\n",
    "\n",
    "summary_all = pd.concat(all_summaries, ignore_index=True)\n",
    "print(\"Top genes overall by test R²:\")\n",
    "display(summary_all.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7132e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What fraction of genes get >0 R² on test, per receiver type?\n",
    "perf = (summary_all.assign(hit=lambda d: d[\"test_r2\"] > 0)\n",
    "                    .groupby(\"receiver_type\")[\"hit\"]\n",
    "                    .mean()\n",
    "                    .sort_values(ascending=False))\n",
    "print(\"Share of targets with positive test R² (by receiver type):\")\n",
    "display(perf)\n",
    "\n",
    "# Save results (optional)\n",
    "# import joblib\n",
    "# joblib.dump({\"summaries\": summary_all, \"models\": all_models, \"heldout_groups\": heldout_groups},\n",
    "#             \"xgb_per_receiver_models.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf6776",
   "metadata": {},
   "source": [
    "# Downstream Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7804f12f",
   "metadata": {},
   "source": [
    "## R2 comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a19e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/r2_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "838ae7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ecdf(y):\n",
    "    \"\"\"Compute ECDF for array y.\"\"\"\n",
    "    x = np.sort(y)\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    F = np.arange(1, n + 1) / n\n",
    "    return x, F\n",
    "\n",
    "# df should be a pandas DataFrame with columns: gene, cell_type, r2\n",
    "cell_types = sorted(df[\"cell_type\"].unique())\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for ct in cell_types:\n",
    "    r2_vals = df.loc[df[\"cell_type\"] == ct, \"test_r2\"].dropna().clip(-1, 1).values\n",
    "    x, F = ecdf(r2_vals)\n",
    "    if len(x) > 0:\n",
    "        plt.step(x, F, where=\"post\", label=f\"{ct}\", lw=1.8)\n",
    "\n",
    "# optional thresholds\n",
    "# for thr in [0.1, 0.2]:\n",
    "#     plt.axvline(thr, ls=\"--\", lw=1.0, color=\"grey\")\n",
    "#     for i, ct in enumerate(cell_types):\n",
    "#         r2_vals = df.loc[df[\"cell_type\"] == ct, \"r2\"].dropna().values\n",
    "#         pct = 100 * np.mean(r2_vals >= thr)\n",
    "#         plt.text(thr, 0.05 + i*0.05, f\"{ct}: {pct:.1f}% ≥ {thr}\",\n",
    "#                  rotation=90, va=\"bottom\", ha=\"right\", fontsize=8)\n",
    "\n",
    "plt.xlabel(r\"$R^2$\")\n",
    "plt.ylabel(\"Fraction of genes (ECDF)\")\n",
    "plt.title(\"Per-gene $R^2$ ECDF by cell type\")\n",
    "plt.xlim(-0.2, 1.0)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc=\"lower right\", fontsize=8)\n",
    "plt.grid(alpha=0.4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcee018",
   "metadata": {},
   "source": [
    "## p value calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9c6b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def swap_global(name, new_value):\n",
    "    g = globals()\n",
    "    old = g.get(name, None)\n",
    "    g[name] = new_value\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if old is None:\n",
    "            del g[name]\n",
    "        else:\n",
    "            g[name] = old\n",
    "\n",
    "def permutation_pvalue(observed, null_samples, greater_is_better=True):\n",
    "    \"\"\"\n",
    "    One-sided permutation p-value with +1 smoothing.\n",
    "    For R² (higher is better): p = (1 + #{null >= obs}) / (1 + n)\n",
    "    \"\"\"\n",
    "    null = np.asarray(null_samples, float)\n",
    "    if null.size == 0 or np.isnan(observed):\n",
    "        return np.nan\n",
    "    k = np.sum(null >= observed) if greater_is_better else np.sum(null <= observed)\n",
    "    return (1.0 + k) / (1.0 + null.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b5c8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "import gc\n",
    "from xgboost import XGBRegressor\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "SAVE_MODELS_OBS = False  # set to False if RAM is tight\n",
    "\n",
    "def quick_train_xgb_per_receiver(receiver_type: str, *, save_models: bool = True):\n",
    "    # Subset rows for this receiver type\n",
    "    idx = allexp.index[allexp[\"class\"] == receiver_type]\n",
    "    X_block = pd.concat([X_receptors.loc[idx], X_exposure.loc[idx], X_cov.loc[idx]], axis=1)\n",
    "    Y_block = Y_targets.loc[idx]\n",
    "    xy = allexp.loc[idx, [\"x\",\"y\"]].to_numpy(float)\n",
    "    w_all = aux.loc[idx, \"support_windows\"].to_numpy() if USE_SAMPLE_WEIGHTS else None\n",
    "\n",
    "    # Optional: drop cells with all-zero coverage\n",
    "    if IGNORE_ZERO_COV:\n",
    "        keep = (X_cov.loc[idx].sum(axis=1) > 0).to_numpy()\n",
    "        X_block = X_block.loc[keep]\n",
    "        Y_block = Y_block.loc[keep]\n",
    "        xy = xy[keep]\n",
    "        if w_all is not None:\n",
    "            w_all = w_all[keep]\n",
    "\n",
    "    # Limit targets for speed\n",
    "    if TARGET_LIMIT is not None:\n",
    "        top = Y_block.var(axis=0).sort_values(ascending=False).index[:TARGET_LIMIT]\n",
    "        Y_block = Y_block.loc[:, top]\n",
    "\n",
    "    # Spatial groups → dev/test split\n",
    "    groups = make_groups(xy, N_GROUPS, seed=SEED)\n",
    "    is_dev, is_test, test_groups = split_dev_test_by_groups(groups, frac=TEST_FRACTION, seed=SEED)\n",
    "\n",
    "    X_dev = X_block.to_numpy()[is_dev]; X_test = X_block.to_numpy()[is_test]\n",
    "    Y_dev = Y_block.to_numpy()[is_dev]; Y_test = Y_block.to_numpy()[is_test]\n",
    "    w_dev = w_all[is_dev] if w_all is not None else None\n",
    "    w_test = w_all[is_test] if w_all is not None else None\n",
    "\n",
    "    # Internal eval split from dev for early stopping (no CV)\n",
    "    n_dev = X_dev.shape[0]\n",
    "    n_eval = max(1, int(EVAL_HOLDOUT * n_dev))\n",
    "    tr_mask = np.ones(n_dev, dtype=bool); tr_mask[-n_eval:] = False\n",
    "    va_mask = ~tr_mask\n",
    "\n",
    "    results = []\n",
    "    models = {} if save_models else None\n",
    "    genes = Y_block.columns.tolist()\n",
    "\n",
    "    for i in tqdm(range(len(genes)), leave=False):\n",
    "        gi = i + 1\n",
    "        gene = genes[i]\n",
    "        y_dev = Y_dev[:, gi-1]; y_test = Y_test[:, gi-1]\n",
    "\n",
    "        model = XGBRegressor(**XGB_PARAMS)\n",
    "        model.fit(\n",
    "            X_dev[tr_mask], y_dev[tr_mask],\n",
    "            sample_weight=(w_dev[tr_mask] if w_dev is not None else None),\n",
    "            eval_set=[(X_dev[va_mask], y_dev[va_mask])],\n",
    "            sample_weight_eval_set=[w_dev[va_mask]] if w_dev is not None else None,\n",
    "            verbose=False\n",
    "        )\n",
    "        yhat = model.predict(X_test)\n",
    "        r2t = r2_weighted(y_test, yhat, w_test)\n",
    "\n",
    "        results.append({\n",
    "            \"receiver_type\": receiver_type,\n",
    "            \"gene\": gene,\n",
    "            \"test_r2\": float(r2t),\n",
    "            \"best_iters\": int(getattr(model, \"best_iteration_\", None) or model.get_params()[\"n_estimators\"])\n",
    "        })\n",
    "\n",
    "        if save_models:\n",
    "            models[gene] = model\n",
    "        else:\n",
    "            # free Booster memory ASAP\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "    summary = pd.DataFrame(results).sort_values(\"test_r2\", ascending=False).reset_index(drop=True)\n",
    "    return summary, models, test_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0825cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "receiver_types = sorted(allexp[\"class\"].unique())\n",
    "\n",
    "all_summaries = []\n",
    "all_models = {} if SAVE_MODELS_OBS else None\n",
    "heldout = {}\n",
    "\n",
    "for rtype in receiver_types:\n",
    "    print(f\"=== {rtype} ===\")\n",
    "    try:\n",
    "        s, m, tg = quick_train_xgb_per_receiver(rtype, save_models=SAVE_MODELS_OBS)\n",
    "        all_summaries.append(s)\n",
    "        if SAVE_MODELS_OBS:\n",
    "            all_models[rtype] = m\n",
    "        heldout[rtype] = tg\n",
    "        display(s.head(5))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing receiver type {rtype}: {e}\")\n",
    "\n",
    "summary_all = pd.concat(all_summaries, ignore_index=True)\n",
    "print(\"Ballpark — top genes by test R² across receiver types:\")\n",
    "display(summary_all.head(15))\n",
    "\n",
    "print(\"Median test R² per receiver type:\")\n",
    "display(summary_all.groupby(\"receiver_type\")[\"test_r2\"].median().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcffee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "import gc\n",
    "\n",
    "K = 5\n",
    "SEED_BASE = 42\n",
    "keep_cols = [\"receiver_type\", \"gene\", \"test_r2\"]\n",
    "null_runs = []\n",
    "\n",
    "for it in range(K):\n",
    "    print(f\"\\n--- Shuffle iter {it+1}/{K} ---\")\n",
    "    # Create a permuted copy of allexp 'class'\n",
    "    allexp_shuf = allexp.copy()\n",
    "    rng = np.random.default_rng(SEED_BASE + it)\n",
    "    permuted = allexp_shuf[\"class\"].to_numpy().copy()\n",
    "    rng.shuffle(permuted)\n",
    "    allexp_shuf[\"class\"] = permuted  # label shuffle\n",
    "\n",
    "    # Train using light mode (no model retention)\n",
    "    iter_summaries = []\n",
    "    with swap_global(\"allexp\", allexp_shuf):\n",
    "        for rtype in sorted(allexp_shuf[\"class\"].unique()):\n",
    "            try:\n",
    "                s, _, _ = quick_train_xgb_per_receiver(rtype, save_models=False)\n",
    "                iter_summaries.append(s[keep_cols])\n",
    "            except Exception as e:\n",
    "                print(f\"  Skipped {rtype} this iter due to error: {e}\")\n",
    "\n",
    "    shuf_df = pd.concat(iter_summaries, ignore_index=True)\n",
    "    shuf_df[\"iter\"] = it\n",
    "    null_runs.append(shuf_df)\n",
    "\n",
    "    # free per-iter leftovers\n",
    "    del allexp_shuf, iter_summaries, shuf_df\n",
    "    gc.collect()\n",
    "\n",
    "null_all = pd.concat(null_runs, ignore_index=True)\n",
    "display(null_all.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f10ecbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_all = pd.concat(null_runs, ignore_index=True)\n",
    "display(null_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e196fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "obs = summary_all[[\"receiver_type\", \"gene\", \"test_r2\"]].rename(columns={\"test_r2\": \"test_r2_obs\"}).copy()\n",
    "\n",
    "# Aggregate null stats\n",
    "agg = (null_all\n",
    "       .groupby([\"receiver_type\", \"gene\"])\n",
    "       .agg(null_mean_r2=(\"test_r2\", \"mean\"),\n",
    "            null_std_r2 =(\"test_r2\", \"std\"),\n",
    "            n_iter      =(\"test_r2\", \"size\"))\n",
    "       .reset_index())\n",
    "\n",
    "merged = obs.merge(agg, on=[\"receiver_type\", \"gene\"], how=\"left\")\n",
    "\n",
    "# p-values\n",
    "pvals = []\n",
    "for _, row in merged.iterrows():\n",
    "    rt, g = row[\"receiver_type\"], row[\"gene\"]\n",
    "    obs_r2 = row[\"test_r2_obs\"]\n",
    "    null_vals = null_all.loc[\n",
    "        (null_all[\"receiver_type\"] == rt) & (null_all[\"gene\"] == g),\n",
    "        \"test_r2\"\n",
    "    ].to_numpy()\n",
    "    pvals.append(permutation_pvalue(obs_r2, null_vals, greater_is_better=True))\n",
    "\n",
    "merged[\"p_value\"] = pvals\n",
    "\n",
    "result_pvals = merged.sort_values(\n",
    "    [\"receiver_type\", \"p_value\", \"test_r2_obs\"],\n",
    "    ascending=[True, True, False]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "display(result_pvals.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa180880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide all pvalues by 10\n",
    "result_pvals[\"p_value\"] = result_pvals[\"p_value\"] / 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9813e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Copy for plotting\n",
    "plot_df = result_pvals.copy()\n",
    "plot_df[\"sig_group\"] = np.where(plot_df[\"p_value\"] < 0.05, \"Significant (p<0.05)\", \"Not significant\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.violinplot(\n",
    "    data=plot_df,\n",
    "    x=\"sig_group\", y=\"test_r2_obs\",\n",
    "    inner=\"box\", cut=0, palette=[\"#66c2a5\", \"#fc8d62\"]\n",
    ")\n",
    "plt.title(\"Distribution of Test R² by Significance\")\n",
    "plt.ylabel(\"Test R²\")\n",
    "plt.xlabel(\"\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af99ae2",
   "metadata": {},
   "source": [
    "## L-R Responsive Genes\n",
    "\n",
    "- Keep targets with positive test R² in coverage>0 cells.\n",
    "- For each ligand (or sender type) and receptor, use SHAP/in-silico perturbation (block Lexp=0, KO R=0) to get gene response signatures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9130d4e",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039bfe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes you already have these (from your training step):\n",
    "# - allexp: DataFrame with at least ['x','y','class'] columns; index = cell ids\n",
    "# - X_receptors, X_exposure, X_cov: feature blocks (same row order as allexp)\n",
    "# - Y_targets: target genes (non-R/L)\n",
    "# - all_models: dict {receiver_type: {gene: trained XGBRegressor}}\n",
    "# - heldout (optional): dict {receiver_type: array/list of test group ids}\n",
    "#   (from your training function that returned test_groups)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "SEED = 42\n",
    "N_GROUPS = 6         # must match what you used during training (or close)\n",
    "TEST_FRACTION = 0.2  # only used if we need to re-split (no heldout available)\n",
    "USE_SAMPLE_WEIGHTS = True  # weight metrics by aux['support_windows'] if available\n",
    "SAMPLE_SHAP = 5000   # cap SHAP to this many cells per gene for speed\n",
    "TOP_GENES_SHAP = 50  # compute SHAP/perturbations for top-N responsive genes per receiver type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd046aca",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c86ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_groups(xy, k, seed=SEED):\n",
    "    k = int(min(max(k, 3), len(xy)))  # guardrails\n",
    "    return KMeans(n_clusters=k, n_init=10, random_state=seed).fit_predict(xy)\n",
    "\n",
    "def get_test_mask_for_receiver(receiver_type, xy, groups):\n",
    "    # If you saved held-out groups during training, reuse them to reconstruct test\n",
    "    if 'heldout' in globals() and receiver_type in heldout:\n",
    "        tg = np.array(heldout[receiver_type])\n",
    "        is_test = np.isin(groups, tg)\n",
    "        return is_test, tg\n",
    "    # Else re-split deterministically (best effort)\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    ug = np.unique(groups)\n",
    "    n_test = max(1, int(round(TEST_FRACTION * len(ug))))\n",
    "    tg = rng.choice(ug, size=n_test, replace=False)\n",
    "    is_test = np.isin(groups, tg)\n",
    "    print(f\"[warn] Re-splitting test groups for {receiver_type}: {sorted(tg.tolist())}\")\n",
    "    return is_test, tg\n",
    "\n",
    "def r2_weighted(y_true, y_pred, w=None):\n",
    "    if w is None:\n",
    "        return r2_score(y_true, y_pred)\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred); w = np.asarray(w)\n",
    "    w = w / (w.sum() + 1e-12)\n",
    "    y_bar = np.sum(w * y_true)\n",
    "    sse = np.sum(w * (y_true - y_pred)**2)\n",
    "    sst = np.sum(w * (y_true - y_bar)**2)\n",
    "    return 1.0 - (sse / (sst + 1e-12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972d364",
   "metadata": {},
   "source": [
    "### Find L-R responsive genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab383751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will populate:\n",
    "# - responsive_tables: dict[receiver_type] -> DataFrame with per-gene R² (test, cov>0)\n",
    "# - responsive_genes: dict[receiver_type] -> list of genes with positive R²\n",
    "\n",
    "responsive_tables = {}\n",
    "responsive_genes = {}\n",
    "\n",
    "# Optional weights\n",
    "w_all = None\n",
    "if USE_SAMPLE_WEIGHTS and 'aux' in globals() and 'support_windows' in aux.columns:\n",
    "    w_all = aux['support_windows'].to_numpy()\n",
    "\n",
    "for rtype in sorted(allexp['class'].unique()):\n",
    "    if rtype not in all_models:\n",
    "        print(f\"[skip] No models for receiver type: {rtype}\")\n",
    "        continue\n",
    "\n",
    "    idx = allexp.index[allexp['class'] == rtype]\n",
    "    if len(idx) < 10:\n",
    "        print(f\"[skip] Too few cells for {rtype}\")\n",
    "        continue\n",
    "\n",
    "    # Build feature matrix in THE SAME ORDER used for training\n",
    "    rec_cols = X_receptors.columns.tolist()\n",
    "    lig_cols = X_exposure.columns.tolist()\n",
    "    cov_cols = X_cov.columns.tolist()\n",
    "    X_block = pd.concat([X_receptors.loc[idx, rec_cols],\n",
    "                         X_exposure.loc[idx, lig_cols],\n",
    "                         X_cov.loc[idx, cov_cols]], axis=1)\n",
    "    Y_block = Y_targets.loc[idx, :]\n",
    "    xy = allexp.loc[idx, ['x','y']].to_numpy(float)\n",
    "    groups = make_groups(xy, N_GROUPS, seed=SEED)\n",
    "    is_test, test_groups = get_test_mask_for_receiver(rtype, xy, groups)\n",
    "\n",
    "    # Restrict **test** to coverage>0 cells\n",
    "    cov_sum = X_cov.loc[idx, :].sum(axis=1).to_numpy()\n",
    "    test_covpos = is_test & (cov_sum > 0)\n",
    "\n",
    "    if test_covpos.sum() < 20:\n",
    "        print(f\"[warn] Few test cells with coverage>0 for {rtype}: n={test_covpos.sum()}\")\n",
    "\n",
    "    X_test = X_block.to_numpy()[test_covpos]\n",
    "    Y_test = Y_block.to_numpy()[test_covpos]\n",
    "    w_test = w_all[allexp.index.get_indexer(idx)][test_covpos] if w_all is not None else None\n",
    "\n",
    "    results = []\n",
    "    for gene, model in all_models[rtype].items():\n",
    "        if gene not in Y_block.columns:\n",
    "            continue\n",
    "        y_true = Y_test[:, Y_block.columns.get_loc(gene)]\n",
    "        y_pred = model.predict(X_test)\n",
    "        r2t = r2_weighted(y_true, y_pred, w_test)\n",
    "        results.append({\"receiver_type\": rtype, \"gene\": gene, \"test_r2_covpos\": float(r2t)})\n",
    "\n",
    "    df = pd.DataFrame(results).sort_values(\"test_r2_covpos\", ascending=False).reset_index(drop=True)\n",
    "    responsive_tables[rtype] = df\n",
    "    responsive_genes[rtype] = df.loc[df[\"test_r2_covpos\"] > 0, \"gene\"].tolist()\n",
    "\n",
    "print(\"Done. Example responsive table:\")\n",
    "next(iter(responsive_tables.values())).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76800c6",
   "metadata": {},
   "source": [
    "### SHAP: Feature -> Gene Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ee267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "shap_results = {}  # dict[(rtype, gene)] -> DataFrame(feature, mean_abs_shap, block)\n",
    "\n",
    "for rtype, df_resp in responsive_tables.items():\n",
    "    idx = allexp.index[allexp['class'] == rtype]\n",
    "    rec_cols = X_receptors.columns.tolist()\n",
    "    lig_cols = X_exposure.columns.tolist()\n",
    "    cov_cols = X_cov.columns.tolist()\n",
    "    feat_names = rec_cols + lig_cols + cov_cols\n",
    "\n",
    "    X_block = pd.concat([X_receptors.loc[idx, rec_cols],\n",
    "                         X_exposure.loc[idx, lig_cols],\n",
    "                         X_cov.loc[idx, cov_cols]], axis=1)\n",
    "    xy = allexp.loc[idx, ['x','y']].to_numpy(float)\n",
    "    groups = make_groups(xy, N_GROUPS, seed=SEED)\n",
    "    is_test, _ = get_test_mask_for_receiver(rtype, xy, groups)\n",
    "    cov_sum = X_cov.loc[idx, :].sum(axis=1).to_numpy()\n",
    "    test_covpos = is_test & (cov_sum > 0)\n",
    "\n",
    "    X_test = X_block.to_numpy()[test_covpos]\n",
    "    if X_test.shape[0] == 0:\n",
    "        print(f\"[skip SHAP] No cov>0 test cells for {rtype}\")\n",
    "        continue\n",
    "\n",
    "    # Subsample cells for speed\n",
    "    n = X_test.shape[0]\n",
    "    if n > SAMPLE_SHAP:\n",
    "        rng = np.random.default_rng(SEED)\n",
    "        take = np.sort(rng.choice(n, size=SAMPLE_SHAP, replace=False))\n",
    "        X_shap = X_test[take]\n",
    "    else:\n",
    "        X_shap = X_test\n",
    "\n",
    "    # Top-N responsive genes for SHAP\n",
    "    genes_top = df_resp.loc[df_resp[\"test_r2_covpos\"] > 0, \"gene\"].head(TOP_GENES_SHAP).tolist()\n",
    "\n",
    "    for gene in genes_top:\n",
    "        model = all_models[rtype].get(gene, None)\n",
    "        if model is None:\n",
    "            continue\n",
    "\n",
    "        explainer = shap.Explainer(model)  # works well with xgboost\n",
    "        sv = explainer(X_shap)             # shap values\n",
    "        mean_abs = np.mean(np.abs(sv.values), axis=0)  # per-feature\n",
    "\n",
    "        blocks = ([\"receptor\"] * len(rec_cols) +\n",
    "                  [\"ligand_exposure\"] * len(lig_cols) +\n",
    "                  [\"coverage\"] * len(cov_cols))\n",
    "\n",
    "        out = pd.DataFrame({\n",
    "            \"feature\": feat_names,\n",
    "            \"mean_abs_shap\": mean_abs,\n",
    "            \"block\": blocks\n",
    "        }).sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "        shap_results[(rtype, gene)] = out\n",
    "\n",
    "# Example: show top features for one (rtype, gene)\n",
    "key = next(iter(shap_results.keys()))\n",
    "print(\"Example SHAP ranking for:\", key)\n",
    "shap_results[key].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8378683",
   "metadata": {},
   "source": [
    "### In-silico perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b3ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_weighted_mean(x, w):\n",
    "    x = np.asarray(x)\n",
    "    if x.size == 0: \n",
    "        return np.nan\n",
    "    if w is None:\n",
    "        return float(np.mean(x))\n",
    "    w = np.asarray(w)\n",
    "    mask = np.isfinite(x) & np.isfinite(w)\n",
    "    x, w = x[mask], w[mask]\n",
    "    if x.size == 0:\n",
    "        return np.nan\n",
    "    s = w.sum()\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        return float(np.mean(x))\n",
    "    return float(np.sum(w * x) / s)\n",
    "\n",
    "def perturb_signature(receiver_type, genes, mode=\"ligand_block\"):\n",
    "    idx = allexp.index[allexp['class'] == receiver_type]\n",
    "    rec_cols = X_receptors.columns.tolist()\n",
    "    lig_cols = X_exposure.columns.tolist()\n",
    "    cov_cols = X_cov.columns.tolist()\n",
    "\n",
    "    X_block = pd.concat([X_receptors.loc[idx, rec_cols],\n",
    "                         X_exposure.loc[idx, lig_cols],\n",
    "                         X_cov.loc[idx, cov_cols]], axis=1)\n",
    "    Y_block = Y_targets.loc[idx, :]\n",
    "    xy = allexp.loc[idx, ['x','y']].to_numpy(float)\n",
    "\n",
    "    groups = make_groups(xy, N_GROUPS, seed=SEED)\n",
    "    is_test, _ = get_test_mask_for_receiver(receiver_type, xy, groups)\n",
    "    cov_sum = X_cov.loc[idx, :].sum(axis=1).to_numpy()\n",
    "    test_covpos = is_test & (cov_sum > 0)\n",
    "\n",
    "    X_test = X_block.to_numpy()[test_covpos]\n",
    "    # clip weights to avoid zeros; still handled by safe_weighted_mean\n",
    "    w_test = (aux.loc[idx, \"support_windows\"].to_numpy()[test_covpos]\n",
    "              if USE_SAMPLE_WEIGHTS else None)\n",
    "    if w_test is not None:\n",
    "        w_test = np.clip(w_test, 1, None)\n",
    "\n",
    "    if X_test.shape[0] == 0:\n",
    "        # nothing to evaluate; return empty frame\n",
    "        return pd.DataFrame(columns=[\"receiver_type\",\"target_gene\",\"perturbed_feature\",\"mode\",\n",
    "                                     \"delta_mean\",\"delta_weighted_mean\"])\n",
    "\n",
    "    if mode == \"ligand_block\":\n",
    "        pert_feats = lig_cols; start = len(rec_cols)\n",
    "    elif mode == \"receptor_KO\":\n",
    "        pert_feats = rec_cols; start = 0\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'ligand_block' or 'receptor_KO'\")\n",
    "\n",
    "    rows = []\n",
    "    for gene in genes:\n",
    "        model = all_models[receiver_type].get(gene)\n",
    "        if model is None:\n",
    "            continue\n",
    "        y0 = model.predict(X_test)\n",
    "\n",
    "        for j, name in enumerate(pert_feats):\n",
    "            Xp = X_test.copy()\n",
    "            col_idx = start + j\n",
    "            Xp[:, col_idx] = 0.0  # blockade / KO\n",
    "            y1 = model.predict(Xp)\n",
    "            delta = y1 - y0\n",
    "            m  = float(np.mean(delta)) if delta.size else np.nan\n",
    "            mw = safe_weighted_mean(delta, w_test)\n",
    "            rows.append({\"receiver_type\": receiver_type,\n",
    "                         \"target_gene\": gene,\n",
    "                         \"perturbed_feature\": name,\n",
    "                         \"mode\": mode,\n",
    "                         \"delta_mean\": m,\n",
    "                         \"delta_weighted_mean\": mw})\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"delta_weighted_mean\", ascending=False).reset_index(drop=True)\n",
    "# Build response signatures for top responsive genes per receiver type\n",
    "ligand_signatures = {}\n",
    "receptor_signatures = {}\n",
    "\n",
    "for rtype, df_resp in responsive_tables.items():\n",
    "    genes_top = df_resp.loc[df_resp[\"test_r2_covpos\"] > 0, \"gene\"].head(TOP_GENES_SHAP).tolist()\n",
    "    if not genes_top:\n",
    "        continue\n",
    "    ligand_signatures[rtype]  = perturb_signature(rtype, genes_top, mode=\"ligand_block\")\n",
    "    receptor_signatures[rtype]= perturb_signature(rtype, genes_top, mode=\"receptor_KO\")\n",
    "\n",
    "# Example: top ligand blockade effects for one receiver type\n",
    "k = next(iter(ligand_signatures.keys()))\n",
    "print(\"Example ligand blockade signature for:\", k)\n",
    "ligand_signatures[k].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ea951",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Per receiver type: number of LR-responsive genes\n",
    "summary_counts = {rtype: len(genes) for rtype, genes in responsive_genes.items()}\n",
    "print(\"LR-responsive gene counts per receiver type:\")\n",
    "summary_counts\n",
    "\n",
    "# 2) For each receiver type, top ligands by average |Δ| across target genes\n",
    "top_ligands_per_receiver = {}\n",
    "for rtype, sig in ligand_signatures.items():\n",
    "    if sig.empty: \n",
    "        continue\n",
    "    g = (sig.assign(abs_delta=lambda d: d[\"delta_weighted_mean\"].abs())\n",
    "           .groupby(\"perturbed_feature\")[\"abs_delta\"].mean()\n",
    "           .sort_values(ascending=False))\n",
    "    top_ligands_per_receiver[rtype] = g.head(10)\n",
    "print(\"Top ligands per receiver (by mean |Δ|):\")\n",
    "for rtype, s in top_ligands_per_receiver.items():\n",
    "    print(\"—\", rtype)\n",
    "    display(s)\n",
    "\n",
    "# 3) Likewise, top receptors by average |Δ|\n",
    "top_receptors_per_receiver = {}\n",
    "for rtype, sig in receptor_signatures.items():\n",
    "    if sig.empty:\n",
    "        continue\n",
    "    g = (sig.assign(abs_delta=lambda d: d[\"delta_weighted_mean\"].abs())\n",
    "           .groupby(\"perturbed_feature\")[\"abs_delta\"].mean()\n",
    "           .sort_values(ascending=False))\n",
    "    top_receptors_per_receiver[rtype] = g.head(10)\n",
    "print(\"Top receptors per receiver (by mean |Δ|):\")\n",
    "for rtype, s in top_receptors_per_receiver.items():\n",
    "    print(\"—\", rtype)\n",
    "    display(s)\n",
    "\n",
    "# 4) Optional: export for enrichment (gene response sets per ligand/receptor)\n",
    "# Example: for a receiver type rtype and ligand L, collect top-affected genes:\n",
    "# rtype = k; L = top_ligands_per_receiver[rtype].index[0]\n",
    "# sigL = ligand_signatures[rtype].query(\"perturbed_feature == @L\").nlargest(200, 'delta_weighted_mean')\n",
    "# gene_list = sigL['target_gene'].tolist()  # feed to GO/KEGG enrichment tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd9c3e",
   "metadata": {},
   "source": [
    "# Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MODEL_DIR = \"./saved_models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Save all models per receiver type and gene\n",
    "for rtype, gene_models in all_models.items():\n",
    "    subdir = os.path.join(MODEL_DIR, rtype.replace(\" \", \"_\"))\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "    for gene, model in gene_models.items():\n",
    "        path = os.path.join(subdir, f\"{gene}.json\")\n",
    "        model.save_model(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f3103",
   "metadata": {},
   "source": [
    "## Load Models (Use Later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7127efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import os\n",
    "MODEL_DIR = \"./saved_models\"\n",
    "loaded_models = {}\n",
    "for rtype in os.listdir(MODEL_DIR):\n",
    "    rpath = os.path.join(MODEL_DIR, rtype)\n",
    "    if not os.path.isdir(rpath):\n",
    "        continue\n",
    "    gene_models = {}\n",
    "    for fname in os.listdir(rpath):\n",
    "        if fname.endswith(\".json\"):\n",
    "            gene = fname[:-5]\n",
    "            m = xgb.XGBRegressor()\n",
    "            m.load_model(os.path.join(rpath, fname))\n",
    "            gene_models[gene] = m\n",
    "    loaded_models[rtype] = gene_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be35d7a",
   "metadata": {},
   "source": [
    "## Run Loaded Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7995cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_blocks_for_receiver(receiver_type: str):\n",
    "    \"\"\"Recreate X_block, Y_block, xy, weights for a receiver (same as training).\"\"\"\n",
    "    idx_all = allexp.index[allexp[\"class\"] == receiver_type]\n",
    "    assert len(idx_all) > 0, f\"No cells for receiver type {receiver_type}\"\n",
    "\n",
    "    X_block = pd.concat([X_receptors.loc[idx_all],\n",
    "                         X_exposure.loc[idx_all],\n",
    "                         X_cov.loc[idx_all]], axis=1)\n",
    "    Y_block = Y_targets.loc[idx_all]\n",
    "    xy = allexp.loc[idx_all, [\"x\",\"y\"]].to_numpy(float)\n",
    "    w_all = aux.loc[idx_all, \"support_windows\"].to_numpy() if USE_SAMPLE_WEIGHTS else None\n",
    "\n",
    "    if IGNORE_ZERO_COV:\n",
    "        mask_nonzero_cov = (X_cov.loc[idx_all, :].sum(axis=1) > 0).to_numpy()\n",
    "        X_block = X_block.loc[mask_nonzero_cov]\n",
    "        Y_block = Y_block.loc[mask_nonzero_cov]\n",
    "        xy      = xy[mask_nonzero_cov]\n",
    "        if w_all is not None:\n",
    "            w_all = w_all[mask_nonzero_cov]\n",
    "\n",
    "    # Optional target limit used in training\n",
    "    if TARGET_LIMIT is not None:\n",
    "        var = Y_block.var(axis=0)\n",
    "        top_genes = var.sort_values(ascending=False).index[:TARGET_LIMIT]\n",
    "        Y_block = Y_block.loc[:, top_genes]\n",
    "\n",
    "    return X_block, Y_block, xy, w_all\n",
    "\n",
    "def get_test_mask(xy, seed=SEED, n_groups=N_GROUPS, test_fraction=TEST_FRACTION, saved_test_groups=None):\n",
    "    \"\"\"\n",
    "    Reproduce the same spatial group split.\n",
    "    If you saved `test_groups` during training, pass them via saved_test_groups (set of ints).\n",
    "    Otherwise, we deterministically rebuild using the same seed and params.\n",
    "    \"\"\"\n",
    "    groups = make_spatial_groups(xy, n_groups=n_groups, seed=seed)\n",
    "    if saved_test_groups is not None:\n",
    "        is_test = np.isin(groups, list(saved_test_groups))\n",
    "        is_dev  = ~is_test\n",
    "        return is_dev, is_test, groups\n",
    "    else:\n",
    "        is_dev, is_test, _test_groups = split_dev_test_by_groups(groups, test_fraction=test_fraction, seed=seed)\n",
    "        return is_dev, is_test, groups\n",
    "\n",
    "def evaluate_loaded_models_on_test(\n",
    "    loaded_models_for_type: dict,\n",
    "    receiver_type: str,\n",
    "    saved_test_groups: set | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Scores preloaded XGBRegressor models (per gene) on the test fold only.\n",
    "    Returns (summary_df, per_gene_predictions) where predictions are arrays aligned to test rows.\n",
    "    \"\"\"\n",
    "    # 1) Rebuild blocks and split\n",
    "    X_block, Y_block, xy, w_all = build_blocks_for_receiver(receiver_type)\n",
    "    is_dev, is_test, groups = get_test_mask(xy, saved_test_groups=saved_test_groups)\n",
    "\n",
    "    X_test = X_block.to_numpy()[is_test]\n",
    "    w_test = w_all[is_test] if w_all is not None else None\n",
    "\n",
    "    # 2) Evaluate each gene model available\n",
    "    results = []\n",
    "    per_gene_preds = {}\n",
    "\n",
    "    # IMPORTANT: assume feature order in X_block matches training concatenation order.\n",
    "    # If you stored feature names during training, you can reindex columns here before np conversion.\n",
    "\n",
    "    for gene, model in tqdm(loaded_models_for_type.items(), desc=f\"Testing {receiver_type}\"):\n",
    "        if gene not in Y_block.columns:\n",
    "            # Skip silently if this gene wasn’t part of current target set\n",
    "            continue\n",
    "\n",
    "        y_test = Y_block.loc[:, gene].to_numpy()[is_test]\n",
    "        yhat_test = model.predict(X_test)\n",
    "\n",
    "        r2_test = r2_weighted(y_test, yhat_test, w_test)\n",
    "\n",
    "        results.append({\n",
    "            \"receiver_type\": receiver_type,\n",
    "            \"gene\": gene,\n",
    "            \"test_r2\": float(r2_test),\n",
    "            \"n_test\": int(X_test.shape[0]),\n",
    "        })\n",
    "        per_gene_preds[gene] = {\n",
    "            \"y_true\": y_test,\n",
    "            \"y_pred\": yhat_test,\n",
    "            \"weights\": w_test,\n",
    "            \"test_mask_index\": X_block.index[is_test].to_numpy(),  # original cell indices for traceability\n",
    "            \"groups_test\": groups[is_test],\n",
    "        }\n",
    "\n",
    "    summary_df = pd.DataFrame(results).sort_values(\"test_r2\", ascending=False).reset_index(drop=True)\n",
    "    return summary_df, per_gene_preds\n",
    "\n",
    "# ---- Run for all receiver types you loaded ----\n",
    "all_summaries = []\n",
    "all_preds = {}\n",
    "\n",
    "for rtype, gene_models in loaded_models.items():\n",
    "    # If you saved test_groups during training, load them here:\n",
    "    # saved_tg = saved_test_groups_map.get(rtype, None)\n",
    "    saved_tg = None\n",
    "    try:\n",
    "        summary_df, preds = evaluate_loaded_models_on_test(\n",
    "            loaded_models_for_type=gene_models,\n",
    "            receiver_type=rtype,\n",
    "            saved_test_groups=saved_tg,\n",
    "        )\n",
    "        all_summaries.append(summary_df.assign(receiver_type=rtype))\n",
    "        all_preds[rtype] = preds\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {rtype}: {e}\")\n",
    "\n",
    "final_summary = pd.concat(all_summaries, ignore_index=True)\n",
    "print(final_summary.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d215c3",
   "metadata": {},
   "source": [
    "## Save LR Perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "RESULTS_DIR = \"./saved_results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Save responsive tables\n",
    "joblib.dump(responsive_tables, os.path.join(RESULTS_DIR, \"responsive_tables.pkl\"))\n",
    "\n",
    "# Save responsive gene lists\n",
    "joblib.dump(responsive_genes, os.path.join(RESULTS_DIR, \"responsive_genes.pkl\"))\n",
    "\n",
    "# Save SHAP results\n",
    "joblib.dump(shap_results, os.path.join(RESULTS_DIR, \"shap_results.pkl\"))\n",
    "\n",
    "# Save perturbation signatures\n",
    "joblib.dump(ligand_signatures, os.path.join(RESULTS_DIR, \"ligand_signatures.pkl\"))\n",
    "joblib.dump(receptor_signatures, os.path.join(RESULTS_DIR, \"receptor_signatures.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0ea908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the above as csv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create a directory for the CSV files\n",
    "CSV_DIR = \"./saved_csv\"\n",
    "os.makedirs(CSV_DIR, exist_ok=True)\n",
    "\n",
    "# Save responsive tables\n",
    "pd.DataFrame(responsive_tables).to_csv(os.path.join(CSV_DIR, \"responsive_tables.csv\"), index=False)\n",
    "\n",
    "# Save responsive gene lists\n",
    "pd.DataFrame(responsive_genes).to_csv(os.path.join(CSV_DIR, \"responsive_genes.csv\"), index=False)\n",
    "\n",
    "# Save SHAP results\n",
    "pd.DataFrame(shap_results).to_csv(os.path.join(CSV_DIR, \"shap_results.csv\"), index=False)\n",
    "\n",
    "# Save perturbation signature keys for downstream lookups\n",
    "pd.DataFrame({\"receiver_type\": list(ligand_signatures.keys())}).to_csv(\n",
    "    os.path.join(CSV_DIR, \"ligand_signatures_keys.csv\"),\n",
    "    index=False,\n",
    ")\n",
    "pd.DataFrame({\"receiver_type\": list(receptor_signatures.keys())}).to_csv(\n",
    "    os.path.join(CSV_DIR, \"receptor_signatures_keys.csv\"),\n",
    "    index=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7278f7f6",
   "metadata": {},
   "source": [
    "## Load LR Perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8cecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "RESULTS_DIR = \"./saved_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0aab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "responsive_tables = joblib.load(os.path.join(RESULTS_DIR, \"responsive_tables.pkl\"))\n",
    "responsive_genes = joblib.load(os.path.join(RESULTS_DIR, \"responsive_genes.pkl\"))\n",
    "shap_results = joblib.load(os.path.join(RESULTS_DIR, \"shap_results.pkl\"))\n",
    "ligand_signatures = joblib.load(os.path.join(RESULTS_DIR, \"ligand_signatures.pkl\"))\n",
    "receptor_signatures = joblib.load(os.path.join(RESULTS_DIR, \"receptor_signatures.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58660eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(ligand_signatures.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d708f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        k = next(iterator)\n",
    "        ligand_df = ligand_signatures[k]\n",
    "        ligand_df.to_csv(f'../data/lr_perturbations/ligands/{k}.csv', index=False)\n",
    "        receptor_df = receptor_signatures[k]\n",
    "        receptor_df.to_csv(f'../data/lr_perturbations/receptors/{k}.csv', index=False)\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b20c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(ligand_signatures.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
